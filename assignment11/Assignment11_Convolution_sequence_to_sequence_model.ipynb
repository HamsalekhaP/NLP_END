{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment11-Convolution sequence to sequence model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXcUrPBaFZ4X"
      },
      "source": [
        "Convolution sequence to sequence model for German to English Translation\r\n",
        "\r\n",
        "We have previously seen RNN models for Sequence to sequence models. They suffer from vanishing/exploding gradient problems. Training and computation is also slow.\r\n",
        "\r\n",
        "This paper introduces the use of CNN for Machine Translation. This improves computation time as processing is done in parallel as against sequential processing in RNN. This has also proved to be more effective than RNN models.\r\n",
        "\r\n",
        "\r\n",
        "This notebook has the Implementation of [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf) paper\r\n",
        "# Convolutional Seq2Seq\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq0.png)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dJRZtnZbaRU"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import time\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqKVEy5BKuRC"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGOIkDZbLv42"
      },
      "source": [
        "!python -m spacy download en\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37pFjpBLCLN"
      },
      "source": [
        "spacy_de = spacy.load('de')\r\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyB10kpLLlB"
      },
      "source": [
        "def tokenize_de(text):\r\n",
        "  \"\"\"Tokenize German text from a string into a list of tokens\"\"\"\r\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "  \"\"\"Tokenize English text from a string into a list of tokens\"\"\"\r\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krw6BwlCLrNf"
      },
      "source": [
        "# Create tokenised list for both German and English\r\n",
        "SRC = Field(tokenize = tokenize_de,\r\n",
        "            init_token = '<sos>',\r\n",
        "            eos_token = '<eos>',\r\n",
        "            lower = True,\r\n",
        "            batch_first = True)\r\n",
        "\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en,\r\n",
        "            init_token = '<sos>',\r\n",
        "            eos_token = '<eos>',\r\n",
        "            lower = True,\r\n",
        "            batch_first = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Kh8NuNRDXk",
        "outputId": "c9e0388f-ab87-44a2-84b0-ac89d09d612f"
      },
      "source": [
        "print(vars((SRC)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sequential': True, 'use_vocab': True, 'init_token': '<sos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'fix_length': None, 'dtype': torch.int64, 'preprocessing': None, 'postprocessing': None, 'lower': True, 'tokenize': <function tokenize_de at 0x7f52f324e8c8>, 'include_lengths': False, 'batch_first': True, 'pad_token': '<pad>', 'pad_first': False, 'truncate_first': False, 'stop_words': None, 'is_target': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3E7TVenPOJ2",
        "outputId": "45849319-9336-48bb-8415-46adf4c589df"
      },
      "source": [
        "# Create train, validation and test set partitions\r\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de','.en'),\r\n",
        "                fields = (SRC,TRG))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 536kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 170kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 162kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsZwruKEQc_z",
        "outputId": "ab589d73-d267-4b02-b0bb-51f864e32f87"
      },
      "source": [
        "for i in train_data:\r\n",
        "  print(i.src)\r\n",
        "  print(i.trg)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n",
            "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJV2ZeeHPwc3"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\r\n",
        "\r\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKf1h30RP7es"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgC8kqXoQF0T"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "# numericalize the tokens and create iterators\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "     batch_size = BATCH_SIZE,\r\n",
        "     device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhSJKhgWQfZ0",
        "outputId": "5a3bc550-2eb7-418b-803a-e6f0a033d928"
      },
      "source": [
        "for i in train_iterator:\r\n",
        "  print(i.src)\r\n",
        "  print(i.trg)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   2,    8,   16,  ...,    1,    1,    1],\n",
            "        [   2,   54, 1552,  ...,    1,    1,    1],\n",
            "        [   2,    5,  717,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,    8,   36,  ...,    1,    1,    1],\n",
            "        [   2,    5,   13,  ...,    1,    1,    1],\n",
            "        [   2,    5,   13,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[   2,   14,   13,  ...,    1,    1,    1],\n",
            "        [   2,   19, 1693,  ...,    1,    1,    1],\n",
            "        [   2,    4,  192,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,    4,   38,  ...,    1,    1,    1],\n",
            "        [   2,    4,    9,  ...,    1,    1,    1],\n",
            "        [   2,    9,   22,  ...,    1,    1,    1]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGkWQIi3RVU-"
      },
      "source": [
        "## Encoder\r\n",
        "\r\n",
        "Points to note:\r\n",
        "\r\n",
        "Pad samples with start and end tokens. Feed it to embedding layer to get embedding of specified size. \r\n",
        "A linear layer translates to specific dimensions to the convolution block.\r\n",
        "This is padded on both size to maintain convolution output size. GLU is used as activation function. This halves output dimensions, hece double the dimensions in the convolution layer.\r\n",
        "\r\n",
        "Residual connections are created by combining the embedding layer output and the output from another linear layer to form a \"combined\" output.\r\n",
        "\r\n",
        "\r\n",
        "## Convolutional Blocks\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq1.png)\r\n",
        "\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq2.png)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgNJ_GyORN2d"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100\r\n",
        "               ):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    assert kernel_size %2, 'kernel size must be odd'\r\n",
        "    self.device = device\r\n",
        "        \r\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "    self.tok_embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "    self.pos_embedding = nn.Embedding(max_length, emb_dim )\r\n",
        "\r\n",
        "    self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "    self.convs = nn.ModuleList([\r\n",
        "                                nn.Conv1d(in_channels = hid_dim,\r\n",
        "                                          out_channels = 2 * hid_dim,\r\n",
        "                                          kernel_size=  kernel_size,\r\n",
        "                                          padding = (kernel_size -1) //2\r\n",
        "                                          ) \r\n",
        "                                for _ in range(n_layers)\r\n",
        "    ])\r\n",
        "    self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "  def forward(self, src):\r\n",
        "    src_len = src.shape[1]\r\n",
        "    batch_size = src.shape[0]\r\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "    tok_embedding = self.tok_embedding(src)\r\n",
        "    # for batch\r\n",
        "    pos_embedding = self.pos_embedding(pos)\r\n",
        "\r\n",
        "    embedded = self.dropout(tok_embedding + pos_embedding)\r\n",
        "    conv_input = self.emb2hid(embedded)\r\n",
        "    # conv_input = [batch_size, src_len, hid dim]\r\n",
        "\r\n",
        "    conv_input = conv_input.permute(0,2,1)\r\n",
        "    # conv_input = [batch_size, hid dim, src_len]\r\n",
        "    for i, conv in enumerate(self.convs):\r\n",
        "      conved = conv(self.dropout(conv_input))\r\n",
        "      # conved = [batch_size, 2* hid_dim, src_len]\r\n",
        "      conved = F.glu(conved, dim = 1)\r\n",
        "      # conved = [batch_size, hid_dim, src_len]\r\n",
        "      # this addition introduces sudden increase in gradient due to sudden increase in amplitude. So Scale this\r\n",
        "      conved = (conved + conv_input) * self.scale\r\n",
        "      conv_input = conved\r\n",
        "    \r\n",
        "    conved = self.hid2emb(conved.permute(0,2,1))\r\n",
        "    combined = (conved + embedded) * self.scale\r\n",
        "\r\n",
        "    return conved, combined\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82qFmsA8wuSq"
      },
      "source": [
        "## Decoder \r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq3.png)\r\n",
        "\r\n",
        "Decoder input before the convolution block is same as encoder\r\n",
        "\r\n",
        "## Decoder Conv Blocks\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq4.png)\r\n",
        "\r\n",
        "In the convolution block, the inputs are padded differently. (kernel_size-1) padding is applied to the beginning of the input. This is to ensure that the kernel looks at only the current work to predict the next word. For example, in the figure, the kernel looks at <sos> to predict the next word \"two\". Also, this is the case with training, but during test time, you will never know the next word until you predict it.\r\n",
        "\r\n",
        "\r\n",
        "## Incorrect Padding\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq5.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgovc1OWv5Jl"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "      \r\n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        #conved = [batch size, hid dim, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #permute and convert back to emb dim\r\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved_emb = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        combined = (conved_emb + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch size, trg len, emb dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #energy = [batch size, trg len, src len]\r\n",
        "        \r\n",
        "        attention = F.softmax(energy, dim=2)\r\n",
        "        \r\n",
        "        #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, emd dim]\r\n",
        "        \r\n",
        "        #convert from emb dim -> hid dim\r\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #apply residual connection\r\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\r\n",
        "        \r\n",
        "        #attended_combined = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        return attention, attended_combined\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "            \r\n",
        "        #create position tensor\r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(trg)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded = [batch size, trg len, emb dim]\r\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #combine embeddings by elementwise summing\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #permute for convolutional layer\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        batch_size = conv_input.shape[0]\r\n",
        "        hid_dim = conv_input.shape[1]\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #apply dropout\r\n",
        "            conv_input = self.dropout(conv_input)\r\n",
        "        \r\n",
        "            #need to pad so decoder can't \"cheat\"\r\n",
        "            padding = torch.zeros(batch_size, \r\n",
        "                                  hid_dim, \r\n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\r\n",
        "\r\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\r\n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\r\n",
        "        \r\n",
        "            #pass through convolutional layer\r\n",
        "            conved = conv(padded_conv_input)\r\n",
        "\r\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\r\n",
        "            \r\n",
        "            #pass through GLU activation function\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #calculate attention\r\n",
        "            attention, conved = self.calculate_attention(embedded, \r\n",
        "                                                         conved, \r\n",
        "                                                         encoder_conved, \r\n",
        "                                                         encoder_combined)\r\n",
        "            \r\n",
        "            #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "            #apply residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "            \r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #set conv_input to conved for next loop iteration\r\n",
        "            conv_input = conved\r\n",
        "            \r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "         \r\n",
        "        #conved = [batch size, trg len, emb dim]\r\n",
        "            \r\n",
        "        output = self.fc_out(self.dropout(conved))\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ah_6wr5BdK8"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "  def __init__(self, encoder,decoder):\r\n",
        "    super().__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "  \r\n",
        "  def forward(self, src, trg):\r\n",
        "    #src = [batch size, src len]\r\n",
        "    #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\r\n",
        "    #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\r\n",
        "    #encoder_conved is output from final encoder conv. block\r\n",
        "    #encoder_combined is encoder_conved plus (elementwise) src embedding plus \r\n",
        "    #  positional embeddings \r\n",
        "    encoder_conved, encoder_combined = self.encoder(src)\r\n",
        "\r\n",
        "    #encoder_conved = [batch size, src len, emb dim]\r\n",
        "    #encoder_combined = [batch size, src len, emb dim]\r\n",
        "    \r\n",
        "    #calculate predictions of next words\r\n",
        "    #output is a batch of predictions for each word in the trg sentence\r\n",
        "    #attention a batch of attention scores across the src sentence for \r\n",
        "    #  each word in the trg sentence\r\n",
        "\r\n",
        "    \r\n",
        "    output, attention = self.decoder(trg, encoder_conved, encoder_combined)\r\n",
        "    #output = [batch size, trg len - 1, output dim]\r\n",
        "    #attention = [batch size, trg len - 1, src len]\r\n",
        "    return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCW1V3AFBhw"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "EMB_DIM = 256\r\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\r\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\r\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\r\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\r\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\r\n",
        "ENC_DROPOUT = 0.25\r\n",
        "DEC_DROPOUT = 0.25\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "    \r\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTK54LtqFV14",
        "outputId": "3b822e17-ecad-4118-9ea7-65abb36956c9"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,351,685 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX94isTKFZBD"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XINP9s74BAfZ"
      },
      "source": [
        "Then, we define the training loop for the model.\r\n",
        "\r\n",
        "We handle the sequences a little differently than previous tutorials. For all models we never put the <eos> into the decoder. This is handled in the RNN models by the having the decoder loop not reach having the <eos> as an input to the decoder. In this model, we simply slice the <eos> token off the end of the sequence. Thus:\r\n",
        "\r\n",
        "$$\\begin{align*}\r\n",
        "\\text{trg} &amp;= [sos, x_1, x_2, x_3, eos]\\\\\r\n",
        "\\text{trg[:-1]} &amp;= [sos, x_1, x_2, x_3]\r\n",
        "\\end{align*}$$\r\n",
        "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the <eos> token:\r\n",
        "\r\n",
        "$$\\begin{align*}\r\n",
        "\\text{output} &amp;= [y_1, y_2, y_3, eos]\r\n",
        "\\end{align*}$$\r\n",
        "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original trg tensor with the <sos> token sliced off the front, leaving the <eos> token:\r\n",
        "\r\n",
        "$$\\begin{align*}\r\n",
        "\\text{output} &amp;= [y_1, y_2, y_3, eos]\\\\\r\n",
        "\\text{trg[1:]} &amp;= [x_1, x_2, x_3, eos]\r\n",
        "\\end{align*}$$\r\n",
        "We then calculate our losses and update our parameters as is standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSx8fIUNFd_j"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        # string eos for target\r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbubjFUKF4iq"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1diwyiriF7PA"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7BhFOZ1Gyn8",
        "outputId": "09ed6b57-c0d2-46e2-a701-4e2c21ada0c6"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 0.1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'cnn-s2s-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 1m 1s\n",
            "\tTrain Loss: 4.422773993487925 | Train PPL: 83.32711392324696\n",
            "\t Val. Loss: 3.2838521897792816 |  Val. PPL: 26.678345057095623\n",
            "Epoch: 02 | Time: 1m 5s\n",
            "\tTrain Loss: 3.1909865755341653 | Train PPL: 24.312401635604584\n",
            "\t Val. Loss: 2.4409987330436707 |  Val. PPL: 11.48450497159517\n",
            "Epoch: 03 | Time: 1m 5s\n",
            "\tTrain Loss: 2.6840327321695336 | Train PPL: 14.644029828563028\n",
            "\t Val. Loss: 2.158005118370056 |  Val. PPL: 8.653857007395327\n",
            "Epoch: 04 | Time: 1m 4s\n",
            "\tTrain Loss: 2.4297406358340763 | Train PPL: 11.355936375046618\n",
            "\t Val. Loss: 2.0100559443235397 |  Val. PPL: 7.463734889238993\n",
            "Epoch: 05 | Time: 1m 5s\n",
            "\tTrain Loss: 2.2686150021490024 | Train PPL: 9.666004144067468\n",
            "\t Val. Loss: 1.927035853266716 |  Val. PPL: 6.8691189586122805\n",
            "Epoch: 06 | Time: 1m 5s\n",
            "\tTrain Loss: 2.1519857261674518 | Train PPL: 8.601922511863428\n",
            "\t Val. Loss: 1.8758232593536377 |  Val. PPL: 6.526189656032853\n",
            "Epoch: 07 | Time: 1m 5s\n",
            "\tTrain Loss: 2.063491407470031 | Train PPL: 7.873411164522693\n",
            "\t Val. Loss: 1.8188984990119934 |  Val. PPL: 6.165063884500401\n",
            "Epoch: 08 | Time: 1m 5s\n",
            "\tTrain Loss: 1.991696150817535 | Train PPL: 7.327952539963822\n",
            "\t Val. Loss: 1.7995874285697937 |  Val. PPL: 6.047152067506441\n",
            "Epoch: 09 | Time: 1m 5s\n",
            "\tTrain Loss: 1.9372322638129347 | Train PPL: 6.9395176124676405\n",
            "\t Val. Loss: 1.7800419479608536 |  Val. PPL: 5.930105169193314\n",
            "Epoch: 10 | Time: 1m 5s\n",
            "\tTrain Loss: 1.8887830521041602 | Train PPL: 6.611318153784541\n",
            "\t Val. Loss: 1.7616382092237473 |  Val. PPL: 5.821967186720108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss_LgYn2Gi32",
        "outputId": "ea65710e-99ea-42b6-973e-2d962e05e648"
      },
      "source": [
        "model.load_state_dict(torch.load('cnn-s2s-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.815 | Test PPL:   6.144 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whGSL-pYta8A"
      },
      "source": [
        "During inference, the decoder is run in a loop, predicting one word at a time, appending the predicted word to the next iteration's input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2GtRqZ0G4dA"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('de')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DCHJCBPHGMw"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize=(10,10))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "        \r\n",
        "    attention = attention.squeeze(0).cpu().detach().numpy()\r\n",
        "    \r\n",
        "    cax = ax.matshow(attention, cmap='bone')\r\n",
        "   \r\n",
        "    ax.tick_params(labelsize=15)\r\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n",
        "                       rotation=45)\r\n",
        "    ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPCFasfbHJw5",
        "outputId": "a5199c3e-6b31-425c-b6f7-bd270d8fd6ff"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['src']\r\n",
        "trg = vars(train_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['ein', 'kleines', 'mädchen', 'klettert', 'in', 'ein', 'spielhaus', 'aus', 'holz', '.']\n",
            "trg = ['a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRX-_ktuHM7M",
        "outputId": "6bafeacf-0e42-4315-964e-c3a0308c3162"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['a', 'little', 'girl', 'climbs', 'in', 'a', 'playhouse', 'made', 'of', 'wood', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "6M-tUR1tHPzk",
        "outputId": "04b9ede6-54a7-44c4-8d69-091c0033a3c7"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAJ1CAYAAACipfqKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5gkVdn38e/NLjkISBBBQCUaQUBERDGAIiCKAdOr6KNrgMfwiAkxRxAVERVRARUJiolgIIgoKAqoqASVpIgEFQRZkuze7x/nNFvbzOzu7M50dU99P9fV125XhzldXV31q5MqMhNJkiR101JtF0CSJEntMQxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkjorImbUf5fq/V/qGsOgJKmTImJmZs6JiBWBQ4HnRcSqbZdLGrSZbRdA0miIiKUyc27b5ZAmQ92e74mIlYFzgLuBi4HZ7ZZMGrzIzLbLIGnIRcSMzJxT/78VcAvwn8y8ISIi3ZFoBEXEMsBZwF3ALOCvmXl3u6WSBs+aQUkLVMNeLwh+FXgKsCJwQUR8ODN/YiDUiHo4cD9g78y8HCAingQ8E7gHODczv99i+aSBMAxKGldfjeB+wPbAO4EHATsBx0XEKzPzBwZCDbvm9lzdDWwAPCwibqHUDv4f8FtgTeDJEXFtZl40+NJKg2MYlDSuRhDchnLQ/HRmfq0uOxt4G3BMRLzUQKhhVweLrAA8GrgAuAI4nDJ45Kb6tJdk5vER8TTgJGCtVgorDZCjiSUtUES8AfglsDtwVW95Zp4LfITS+f6YiHh6ZmZERDsllRbJpynb7I6ZeSfwMWBPYB9gpxoEA7iZsr07aErTngNIJC1URBwNvAz4EvD2zLy58di2wNuBZwM7Z+YZrRRSWgQRsRrwDUp/wb2BM5tNxxGxPLAJ8HlKM/JTHEWv6c6aQUn3Gm/S3czcGzgBeDnwgjodR++xXwKfAI4DrhlAMaVF0r8913kFbwaeC1wGfAV4WkTMrI+vSukTexTl+LhTZs6NCI+VmtasGZQE3GewyB7AAyjNZH/LzEvq8u8AO1M62R+bmf9pvH652uwmDY3aR/B/MvMz9f7MOr/gKsB3gU2BVwJnUmoEXwbMAd5bnzczM+9pqfjSQBgGJdEc+BER3wSeCNwJrAGcD5yYmYc1Hn8m8CbghMy8tZ1Sj61/xKiDWrotIvYH9gcOycwD6rJeIFwL+BGwPPCGzDwtIpbPzDvq8/pHH0vTklXfkmgEwQ8AWwF7AZsDmwG3AofW0ZVk5vOBU4AvAHsOw4CRiJgREY+OiFUatZvPgXmfTZ31deBo4EUR8RGAGgSXzswbgeMpNYLfjYjH9oJgfZ5BUJ3g1DLSFBuxy7htDfwQ+GVm3lGb0nak9KH6ee+zZOZeEXEHcN6QhK2NKeH0NOA9EXEqsGNEPCQzb2i3aBqUsWryMvMvEXEQMAN4YUSQmftn5n/rU+YCH62PXzjYEkvDwTAoTaG+fngPzMy/t12msdQO8itSagN/UYPg5sC5wA+A/83M2yPipRHxp8z8VR1UMiz+BBwLHBIRzwdWAJ5oEOyORtPv8sCTgI2AXwN/zMy/RsTH6lP3iogVKU3HmwK7At/OzEPr+9g0rM6xz6A0RfqC4Gcoza9vyMwL2i3ZffoINv9/DOUA+Ubge5Trtv5PZv6nTiGzP2WS3h8OSY1g/3q+kjI59tGUdT27zbJpMHrbQB3l/hNKRcfawLWUE4X9MvPaiFgfeD3wasrJz83A9cA2DhJRl9lnUJoCfdfzPRF4OvBN4N+tFgyofaV64W8G87cQnAz0Dqi/yMwX1CC4OuVSXQ8EfjdEQbC5nl8EXESZVHhv4B3NKXA0fdUguDylm8DNwJ6Z+QBgGWA34MsR8aDM/CtwIPBk4IB627rWKI45rZLUBTYTS1OgEbb2A7YAXgr8OjPvrgetpYA7BtWXMCKWAdbOzGt6faUi4kBKH8GbI+LEzDw+M0+IiEcCLwGWi4itKYNInkE5qO6QmdcOoswL0+yLGRHfoDQLzqI0DV4DHFweioN6I55rEP7veO+pkfb/gP8Ar8vMK+o2sTpl8ujnA4dHxKsy8zpKYPxd74U2DWsUTeZMCYbBDurfgHoHVafgmBIbA3/IzPMAatD6ALAu8NuI+Fxm/nYqCxARS1P6/d0QEQdk5pURcRSwE/CrWpavRcSGmfmxzDwgIq4HngWcTQlWfwGekJl/mMqyTkQjCK5IuVLE+yiBey7wqTrI+WBgTkQcAtxBCQS/zcxD2im1ptANwPE1CB4CbAfskpm/qzXbrwSOiIh9ag3hvQyCGjV9J8OrUyZSvz0zv74472cY7Ji+/mGrUTpPrxoRX8zMu9ot3fRRr2gwB7gJWC8i9gXuT7ls26+AC4BXALOBKQ2DmfnfiDiv/r23RsRxwErAyzLzxxHxIMp1WT9Sa84+mJmHRcTnKP0Hry1vM2+C6WEREYdSajFvBD7SrGnNzGYg3AG4i9I8+NkWiqpJNM4I/ZOBpSPiAZRuGR8DLq2PfRHYhVIT/uZ6k0ZO4xg+IyLuR6lcWBt4HnBXRJwNXDvRih3DYEc0NqBl+jagPYArKDVHV7RYxJHW38zU64weEd8GHgHsR+mo/q7M/FR9DGDriFh2qoJ4YyqYd0XErcAbKE1nG1CaU8nMayLiU8A9wPsjYm5mfrjWFv9pWGtNauD+JSXobQSsUpff+13UQHg9ZdDATcBWmXlxS0XWEqqj3pfqzRMIrAPcnJn/qeHwrohYD1gPuL7RJeDRlG3leOBbbZRdmgyZmRGxI2Uu2OcCf6VUKNwGfDoz/7Y472sY7Ii6AT2VcvbwPOBKygY0m9q00mb5RlnfaNb9gAdRagGPyswz68CGVYBlMvPq+rw1KIHsMkoN4pTLzAPrAfSdlOD3IOqAlsy8oY54BnhvlEvLvXuYgmB/bVANBN+jNP8eQmka3qE3GKARCI+LiJOAe6z9Hk0R8TDKZRFvBebWgUEnUbbhFSPio8B3MvMa4GLKidcrIuIvwHKUPru/ysxv1Pezj6BGTkS8DtiWsj2fBhwGfIgSCjcDTq/Pm3CXL6eW6YCIeD2l9mQvSg3gLzLzQxGxOyUYvCczz7DP4JKptYCPpQS8lSj9BY8DjsjMZmf1x1CaZZ9FmQvv0jHebjLK0wypj+j194uIN1Ka0L4BfCgz/9x4zVqUbWJvYKPM/NdUlG2i+j7L5nXxvzPzuhpwd6NMOn0J8JRaqzlKk31rHFEmPv85ZZT7IzPz1og4lzJZ9PcotX57AF8BPpuZl0XE9pTte23KFXSuAB5fu0y4n9NIiYiVKJf/fAvl8qCfA36Smf+uj/+QcszZYXG3bcPgNFYPkp8EdqcElEOBczPzlvr494FVM/Px7ZVyeqhnbPtTztAuzTIdy8GUvkmvAr5SA8r7KP3W1gL2aobESS5PMzx9jtJUfWxmHl6XvR34P+DbwCcy8/LGa9cEyMx/TEXZJqqvo/TRlBObVShh4COUmu0bolx+7vOUbd1AOE1EGQm/M/BxykChnYAPA5/p/X4i4l2ULhDfBg6ug0jWoFxD+xbglCzTz8xM5xPUCKq140np/nBz76Sm7vcOA16cmWcv7j7PZuJprJ4FH0rZUG6sG9BSABHxLODhlBF2NpssuYcAfwR+n+XqHQ8GXkbpo3R848d5CvAv4KTM/MtUFaYRBI8HHge8g9Jnqvf4gVE6Lf5ffd7Bva4CwxICexpB8IuUIP1WygF+E8qcgltFxFuAUyl9Aw8FLoyIrQyCoysiNqN0rfhdRJwG/JcS9i+lDGp6d++5mfnhiJhD2Z4zIj6dmX8Evtp4vxkGQY2aiFg3M6/NzEsay4IyPdkc4ImU38PlMG9/OVGGwWkqItYB/tXXBBhA1LtPpHSo/xM4tcIkWBtYowbBDSjXOD0dmFWXvQ64IjNPi4hfDyKkRLks2+Moc++dUWvKgtIBf05mfqwOYvlfYKWIeG9mXjnV5VocdVDANpSBT9/NMl/jzyhh8F/AbXXZKZSJht9P6U82ZYFbUyfKVBlfBFaPiBdn5kUR8RNK94oDgIdRatevj4hlMvPuxvb8BmC1iHhrszO9+ziNmoj4OLBhRBySmef2ltem4DkR8QjgdcC+uYTzv3oFkmkoIj5N6VS6fXN5FnOizHW3D/D52uFai6hXs9q43wvX5wIr1P6Zv6F07p2VmbMj4iGU5uNHTEWzZUSsUGtR+m1AOeG7uPc3607k3r4hmfkxyqXbtgVun8xyLYnaxaHpAcCjgMtr6Nsc+BtwImWE9h0RsUVm3g18h3JVCYPgiMrMm4CjKDXAh9fv9i7K5RE/RJlT8OsRsUrdHpapr/tYfd3KwFBeB1xaFBHxTcpx48eMsS3Xbf5FlG4xpyzp3zMMTjNRZt3fDfgDpdmy//EZlM7Wl1FHHmnR1GamXpPlQyNiVcpIRSgjG+dQmuQvBP5fZt5SB2TsT6ml+vYUBMGgNIteEhFb1mW93/ValH51f++VH+Zrdt213n8XsF1mXj+ZZZuoiJjRGxyS866S8vT68N+Aq4AH174z51K231dm5u21FvRDUS45dlcO4ZyIWjS97Tczj6R0lJ8BfL4GwjspB8c3AssD50TE/foC4buA3Xt9Rtv5FNLii4j3UK5ctRdlVoqrImLpiFi28bS5wGrA+ZOx7/aHMo1ExP6UZsEXUWr9/t7beHo7Skp4eSRwTmZe1U5JR0/trNvrh/cFypnYr4GPRsTDs1zi6hmUwLI2cGBEvJdS6/Yc4AVZp5WZTLWm7zOUawmfGhGPaQTObwBrUDrbz9dMFhEPB94cETvVRTdNdtkWw1bAIbX/H1Gmg3lbbTK8gdIn5gPAOcAZlO18dh0osAulC8RtbRRck6eGuN6JyzGUk51mILyLsr3vA6wAnN2oIZxZX5f1N2uf0SnSaBXR5HsI8LPMPD8z76wnyUcC34+IgyJipdr/9fOUZuIl/j4Mg9NE3XluDJycmb+qG9BmwDFRhp0fFRFrZ+ZsSk3VO+rr/EEvRK0R7F215UDKCMWjgJ9RAuBnawj7C2WAw9mUKx3sQunYu31mXjRV5avvvQ+lRvLkiHh0fegK4MvAyyLi3s72tT/pmygT9l5S32MYphW4EbgaeHtEXARsSZlK4T+1fC+sz1kV+CalZmhLyhVGngW8NTNvbqHcmiSNENicwP0YylVjZjJ2IFwe+GNErNgcIDIk2/S0FGVUdm+fuGbb5ZkuauvIMpQLA6waEbtGxDsp+/ZNgH9S+ni/AyAzf9/b5pd0e3dqmWkkIr5EmXbjtcCTKBvMeZSLsm9MqVF5Q+1XpYXo798XERtSgvRZmXlcXfZyygFpDrBPZv667ijvaTR3TUntRMw/fcyLgIdSas4up0xb85uI2JRy9ZOXUvoy/odSm7I58NSpDKmLqjmoJSLWpzQBrwscmplvqs9Zptb8rENpkr8fpRn8amBZ4IXD8Fm0+Bq/m+UoA9wCuCXnXdf7JZSTmHuA12Xmb+uBcxfKJQlf5CCRqRf3nbbqTuCd6YTukyYitqC0PiXlwgBfy8yD6r7y65T93+6TeWwxDE4jtSbw85SJjy8GvpmZH68b0HcoJw/PabOMoyDK1Q3Wy8Zk0BHxQcpkzJcDe/cOUPWxl1JGMM4BXpNlKoyBTWwbZbLrTSnBf1VgR8p1eHfPzAtrgNoa+B9K7cqfgMMz80+DKN94ai3Q8pl5W2PZC4CnUoLe44GPZ+bB9bFeIJxRH9uYMs3I1bWZXiOqd+JVf3s/pUyguw5lUNNJlNGSd9ZA+EbKNDOvzzLKeOlGH1OnyJpCfUHwW5QuMLcDj8ohnYlgFETE84D1KfNonpWZF0fE2pSBUJF1VpCIuD9lurJLgTdO5jHGMDjCImIvyjU47wAuzMxf1uWPolyv85p6/36Uvms3APsCc2xCGVutzTuWMqnxro2mkHUpc5Y9mVJN/8VmDWsNhK+ndOh9Xg7o+rcR8ULKoJW9KDuRuRHxRMpkzBsBz8zMX/e9ZiiuwBARz6A0A7+89m/9FqU26BWUMPgWYE9KIPx4fc0y1mxPT7WW70zKSdVbKQfGtYEfUn6Tr2oEwn0p/WF3ycaE6aMoIlbPMnp6aNUKhXvnaaz9ebegtER8AHhGTtEE+tNdlEGf2wG9GRRWo+z7jszM2xvP2xR4O7Ar5cpV9xkguiScZ3BERRl2/iTKWdkDgRsi4luZ+aac/9JnW1I6mO4APCGddHWBaph6D3Bt7YT+oMy8JjOvrQeh71CaXS+KiJ/nvClbjqmDdV5Mud7zoNyf8ju+tNFk8DNKc9oJwDcj4jm1tnLYrsZxI6VvzO8j4teUueOek+UKObdExCcp4XC/iJibmZ+gHJfeBVyXZbSpRtA4oX5zSsB7M3BB/f3tVR/7bZaRxGTm12sN4taUAVsjK8pVimZExAnN1oZh0Wu6ryePvSD4A8p3tTtl/X+UUks/0BaR6SDKFam2p1yg4LeU2vBXU661PpMyoC4olw/dmVJjvvNkB0EAMtPbiN0oM+//ldKvZnlK9fKnKAfXzzWe9zpK0+EfgUe3Xe5F/GzRdhkaZXkbZfj+oxvLHkDpzPtn4AmUvm7N16wyBeVYBXjbWOsJeAFl6pgnjPG6g2v5b6U047S+Tsco41aUE5q7gT3qshmNxzehXHP4Zsp1nr9G6aO0edtln863qfwdUg5+xwGr9S1/AqWVY7t6/4V1+317vb86pS9s//vNmKqyTvE6/gZwJaUWdJ1Brf8JlG9Zyjye728s263uT7ap91cE/kGptW19nY7SjTI5/inAV/u/e0r4+y9lvtTefvJ9wIOnqjyOJh5NW1KmNflFZt6RmX+lnJ19EXhWROxVzyb+AhwDPD2HtHN9b/RgT9Ytf0j8lDJi8czaoZcs8zn1rnd6NPD4aMxllpm3TkE5ng/sEhErNP5Obz2dSTlgviUi7tc3OvxvlP5WP6D0IRwajXW2KqV/6+XAlyPioVkGkiwNkKVf44GUK408gjJf42Oz0Z9TS27Av8MVqddX7Vt+G6X2afUo11s9ljKheO/SibsCr4pyqcdmWUeuj2Dtg7wtJfB+NjOvi8ZE65mZ0f4ciatQWh6eHRFvBcjMU4CNM/P8Wgs4G7iG8rsEICJWiog9olyJSeP7L6VpeJUxHvsMpRLnf2qf2AuBD+RUTgfXdjr2tug3ylRAywC/AI6ry2ZSa6coIzD/Any6+Zq2y72gz9P4/yzgg5Tg86AWyjJm7QLwGEogvBnYorF8bUqt603UmowpKNMqlEEfazaWbUyZS3JlYNm67CmUpunjgEfUZWtRRp19gnJ919a/77HWM2Xey/tTaot+SZk6YaP62My+564I3K/tzzDdbm39Dimj2g8CHtpYdiSl5ncupYN8b/lmlJOzLzIEtWZL+LmXpdQKfqixbCPKdFUnUJsIWy5jr+VhHeBblBO2d/Y/Xv9/OvCN+v+VKSfJN9FX2+ltzPX7Wcr0Y48Y4zm/oAwCHUyZ2l4p3hbxiyp9BXob0H6Ump7t6/2ZjcdOBb4/XrgZxlvdMd5YfxT/pNRkbTnAv99sltwReBqlg25v2SMpcwf2B8J1KJfH2miKyvUW4CeN+1+gNE/fRqn1+z/ggfWx3eq6uwL4HXA+ZRqZoWlO7VvPTwZ2AnZoLNu2lvuf1OYQSlh8K6U2sPXPMJ1vg/4dUkaEz6WMjux935vU+3cDzwMeTZlD8lfABb2QxOgHwu9Tpvp6JGWWgjvqwf/H9Tf+oSEoY6+S4YHMC4Rvazy+TP33KEogXLbuo24Ftmq7/MN4q8eMtandiSgnuJdT+nk/pPG8B9RjzscplUBTvr23vnK8LdIGdFC9Paze35Qywu4iYNvG89ak1FYd0naZF/J5mqHgqbXMO1L6P76q3v/lIHYozH+GezylL+Y/6875eOAx9bFHUYLfzczfh3DKQnfdCfR2uF+jBL3n1/vnUwLhgcAD6rL1KP1KjqI0M2zW9nc9gfW8ZX3scfX7/xdlvsyvUkLtJm1/hul2a/N3yLyT150pJzcnUqZz6u3fjqy/tX9T5sc8BVi6v9yjdANe2diHP45y0ja7/rt/Xb4ipSXi6BbLGf3/p7Q6fZu+QFgfexNl8vrj6+95YCfyo3Sj1GpfQKk1PZ3az5IyFdyVlCm/3l/X5zfr9r/pwMrX9gryttAN6BuUM4f30Kh2p8zv9JO6s9wfOIDSTPjvYQoBjfL2zjKbO5r3U6YlOIpGswhlmpQpDYT0nWlRmmb+Wg+KW1BGy91KOVvfvD5ny/ojngs8coDr7qV1J7JDvf9mSn+TH1IGXxxIbdJrrOehOGBOcD1vVp+zLaWv43WUvrFbDLrc0/XW1u+QBTR7UiaNnk2pfVqvsfzR9UC5EfNCSavNp0vw+R9X9xtHARvUZSsD29BoWaB0Dfk+pXvHQGqE+srZPEGYQenT1quNfRBjBEJg7/rZZvtbHXe9fpXSt/I1lHkye4P73lUfXxP4HiVU/5VybB/ooL/WV5K3BW5AH6H0AdwWWLEuW7bx+Cb14HoDpdborEFvQIv4OVYA/kAjpFKavf9YfxAnc99Rub0D0bk0aj8noSzN/lG9A0zvbPydwHKN8s0GjgBWaLxmmxpUBnfGBv8P+GT9/6xarl4N4TcoZ+Mf6R1kmp+txe98Sdfz0pR+Ymu0+Tmm062t3yE1YNS/dShlUNsR9be0Un3smXU7OJFxRkz2l23UbpSrpMyl9KnbeIzHH17Xy420UBPO/EHwA5Rwfh4lmD6qLm8Gwt4o75n1OQ9vex0P440yEvgSSneHXu32Y+u28CXmP6avRWlKXnng5Wx7RXkb54spZ4gnA+9pLHswcDh1lB3zAuJa9UC7UtvlHuezbA58kr5pV2q5f0BpJnom9x1c8HzgMuCM5g9mCcqxQt3ZPqtv+QNrGfat9zemVNF/oxdQKLUXvTPkJS7LAso43kCWB1JqEi4CPtTbWVD63t1OaXp433ivH/D3vaTrebm2P8N0vLXxO2TeicAKlBaOyygDQf5EOYl9L7XFo373t1GaGx+6OJ9xGG+UQX+99dALhEcyf43gvsDP6zpqdRowShPl34GvUJrnr6TMnvDk+vj6lEB4EfDeuqz1/c6w3ihdL25j3pRJG9X99dcb+73Wm9ZbX1He+r6QRqBj3mCQx1AGC9xOabK5oO5M30xpShj6HyLzj57avrF8zboTvKr+aPoPRM8BNpykMjy6/ihPp0y301u+MqUD7ycoZ3E3UUb19Tr57kjp7D2lgxiYvzbtjZTA/5rGsk1r+XdvLNur7pg/xpD0q5uE9bx1259hMT7z0P8GazkH9juk0WWBEjK/T+nX2usHezjl2tLvZl4N4dMpYemDba+rSVjXBzKvi8nS3DcQfpnSurMUZbDM/jQGEbRU5r0ozZlPbJT3CZRj0e3Mm/duXcrJwXnA6m2v62G7Mf9x/Gl1f7hB/Z319nu9bX5PygnQ2q2Wue2V5q3vCylnzZ+o/382pS/V7ZTmnQPq8qUpfamOaLu8i/B5mn2T1qfMsn4LddLSunzN+jmvHutANJnloPTduZwyau8Zjcc/RJnj7HbqNAl1+f0pVfk/BdYa0Do7ltL380+US3N9G9iQEqb+BJzYWJ9fBr7U9vc8iut5Ej9zs3ltB0rwWYl5zaOtj3xt63dIGQ1+NCXkf5cyoW7zpOcLwPU0akYo3WJGsm9g4zNsQQnWf2beVEnNQPgmSiA8lBoAp2K/txjl3odSY7t+3/KtKQN5vse8EPMAGn08vc23vu49jtf759b94c3199Br2VmbUkN4HC00Dc9X5rZXmrfGl1H60FwMvLLen1l33I9n3hlmUK5d+EPgw/V+6webcT7PfXZulCryH1FGiD62sbx3IPozZcqRSe0fxPwd45/EvD6Wz2wsP7zuoA+g9N95MqV/002MMQ/UJJateXDcgBKgtqXUojyNMur2TMrl2l5N6VN0K6XJ7SaGqJ/oMK/nAXz24ymjoOfWHf+sxk6/td/ooH+HzB+ON6AEz38BJzWWL9P4/xXA5/vXE6MfCJ9OGfV/ObWPIPPmBt2QEoLnUmppl26hfEv1/5/S2vQP6glZ3/f0UcoMBvdve90O8415x/FXMC/8705p0buV+adROooyUK71QZ+trzhvjS+jHBz/zhidixvPeRjzOhmP+7y2b30HhFdQLrD9NkoH5C0pgzD6D0Rr1IBzEY3BBJNQlv5pTXamBOwrKMGrGVQ+Uw+Esymdfn/FFIatvvW0TN2R/IjGpboonY3/SelDujXlouYfo/S3Goqm4WFfz1P0eZdu/P/VlIEYz6U0gZ9KGfz1dupE2bQQCAf9O2Rev9oVKBOmr0sJPmdQ5kZ9Q3N7odQcngcc1fb3OUnrewvgGcybQmYHSivO5c3fat1GDgVe1HvugMvZDHmrM296qrUp4eSbjcd7geY1lP6DD2h7PQ/zjTGO43Xf/kLKidFNdZu4kFJ7PBQjsFsvgLf6RZSRk9cBb6n37zOtAOWs7cd1AxqVaw1/izJU/hJK89N1lPnjdqF0Tr73Opf1+fdnEvoIUuZKe3zj/gzKKK3rmTd3YC+onAXs0njuJpTA9RD6rp86hevpMEqfrVMo/ep6TTG9psZtKYHw1N5BpX/7aOn7Han1PAmfd0XgSX3LXkyZXqN//rXj6rb/DloMhPXvTvnvsLGtrlwPdmcB+9RlD6fUbv+BemWRuq08hBIwDm77u52EdfxV4PeULhDnU7vxUPrcXUipVduxbvNHUCYVHljTcP9vtS47Ari0bg8fpQTDV1Bqco+h9kmv28OJzX2TtzHX8VjH8Xu7i1BOkvalnJS9kBautjVu2dsuQNdvjQ3lpXVH3Rtx1Ku2vx91yH494LyHERlpR5lC5BpK/7G167KTKWdGu1BqJk6rO57HT+LfDUpfpBuAnRvL1qrLmge9MWuuBvW91/8fRrnqw9dqOebrQN/YFrapj32PFpqVRnE9T8HnPZYyyrL3u92xfidzgf+ry5pTRfQC4VuBVVsq98B+h/Vg9+v6ftvQGBlOCYRn1HX1fcqJzRmUgKeuGz0AACAASURBVDjqTcJHUUL2MyjB6aT6Ob9dH9+y8dlvoJwsDeyEfpzf6qcpFQsfqfugO+s+aGtK38HeFY3OoYTAmxmx2vsBrt+FHcdXY8in3mm9AN7uPXv4I/D1xrKVKR3Rf1B3IP9bf9Cth4AJfK6v1p1kb/Tgg+rO6Fjmzbf0aMoI6b8yiVOKUEbenkY5U396XbZKPQA+gdIfs/cD3r7u9M6gbzqUAayjx1BGHT6z3l+X0hd0LnXAUG8baTx/mJqGR2I9T+LnXZd5Uzr1JsmeVQ/uP2o8r9kM9zXKaMI30U5T8cB+h5QT1ktozMNJuXrPTnXbXat+//+mhMGdGs8byUBIGWn9a+Cp9f4bKZfT+xKlO8+JjefuRgmMbVx/vflb3ZXS1WSPxuNPr9vpcZQark0pTdnHUeazHdjcqqN4Y+HH8XsY5/rOw3BrvQBdvjGvL8YrKf1mepfk2p9y5jyn/hBfwwiFwPoZlqKMqPpOvf8QypnlCcybW+lVwEMp85+tPwVl2IjSNPX7+oOcUQ9297l6COXM/WbKiMcVB7SO3krprP13GgMnKP12PsA4gXDYbsO+nqf4u+vNvfYaSs3KVxrPaQbCL9FCH99B/w4p103/PfO6C7yZEvxuYN5J7Vp1ezmXOliuV9a2v9fF+LxBuZrOfvX+/1AmgX9BDQKfr5/7G8Nw8K/f85mUAQ731k4y74TtqZQ+pN+hMdXJMJR9WG9M7Dje+ojxcT9H2wXwllBGk11GqbY/vx5kDgee0ve8kfpB1s9zHuUstDe3Uq/v1GaUPnIvmuIy9ILKxZSmjyso/TWeB+xB6eC9HWWQxvNpTAQ7gPWzGWWwyD3AC/seW4sSCO8GPtr2dznK63kKv7sfU64r2wyEdzF/IJyyCconUNaB/Q4pNYx31b93AaWm6X/r9/5+ynQ2K1JOCs6s+7t92l5HS/iZV6KMwl6pfu4PMS9oP5TSBWQuNZC3faNM9n5aLdOLGst7rQ9PpXQZ+Am13+ioHXtaWq8jfRxvvQBdv1H60fT6HH0P+BylZqjXfBPNf0fpBjyiHgzmUs40eyMN16DUlPyOATSX1KByOvMuu3VaPSjdSunUfUu9rdvCOnowpYbkKhrz8dXH1qJcw/JmRuCybMO8nqfo8z6UEgj/wH0D4ZFtl69RzoH+Din9Q4+k1J4+prH8DZTm1NUb5bqgho77tb2eJuFzP7Bu5/s1lj23/r5fRcsTSveV9SFj7XcagfAZlNp95xFctPU58sfxXgHVkohYgbKj+DdwcmbeXJdHToMvJyKeRmkS/CVluhEondafTBmV+bsBlWNjSifpjSmjPk+MiLUpzWjLArMz8x+DKMsYZXsoZVTfmrVsP2w8tgZlB9JK2SZqmNfzVIiIjSjf3VrA/2bmWRHxakpn/cMz8/WtFrBq83cYEUtTgvMXKbVkL87MufWxh1G2ib9M1d8flIjozdH4W8pAvzsoteMrAq/NzNktFu8+xtvvRMRSmTk3IlbIzNtbLeSImA7HccPgEIiIGZk5p3F/ZDagRRER21CmLViP0n/ij8C7M/PiAZdjI0ofnnWAN2XmGYP8+wtSy/YFSqh4S2ae1nKRFtswr+ep0BcI98nMsyPiFcB5mXlpu6Wbp43fYUTcn9KPbndKM+o2mXlPRMwA5k6n/RxAROxA6foxmxIGl6cMLBnISe9ETaf9zngi4gmZec4A/s5IH8cNgxqIiFieMvHmHOC/mXlXS+XYiFKF/zDg5Zl5ZhvlGEst22cpzWcvG6ayTdQwr+ep0Pi8Dwf2GsTBZ3EM+ncYETtRJke/CnhFDYIzM/Oeqfy7bYqIR1L6yd4BfC8zL2+5SAs0nfY7/SLiqZSuK2/NzE+0XZ5hZhhU50TEpsBBwJsz88q2y9M0zGWbqOn0WRZF/bwfp9SGTvvPuygiIihT2VyTmdlfe6LhMF1/qxGxKvB/wLGZeVnb5RlmhkF1UkQsk5l3t12OsQxz2SZqOn2WRdG1zzsRvb5obZdDY5uu267b3aIxDEqSJHXYUm0XQJIkSe0xDEqSJHWYYVCSJKnDDIOSJEkdZhicJiJiVttlWByjWG7LPDijWG7LPBijWGYYzXJb5sFpq9yGweljJDd8RrPclnlwRrHclnkwRrHMMJrltsyDYxiUJEnSYDnP4IBFxMit8K222mrK3vsf//gHa6655pS894UXXjgl7ytJ0qjJzBjvMcPggI1iGBzVbaRcCUuSJC0oDNpMLEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDC6GiNguIk6KiOsiYnZE/DYiXtJ2uSRJkiZqZtsFGFEbAOcChwN3AtsDR0XE3Mw8rtWSSZIkTUBkZttlGGkREcAM4LPAxpn5lDGeMwuYVe9uNcDiTYpR3UbKVyNJkjJz3IOiYXAxRMRqwPuBPYB1KWEQ4NrMXG8hrx25FT6q24hhUJKkYkFh0GbixXM08Djgg8AlwK3A6yjhUJIkaWQYBicoIpYDdgP2yczDG8sdjCNJkkaOAWbilqWst7t6CyJiZeBZrZVIkiRpMVkzOEGZeUtEnA+8JyJuBeYC7wBuAVZptXCSJEkT5ACSxRARGwFfoPQb/BdwGLACsG9mrrGQ147cCh/VbcQBJJIkFY4mHiKGwcExDEqSVCwoDNpnUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYTPbLoCGX0S0XYTFkpltF2HCRnVdS5JGlzWDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHTbtwmBEHB0RF9T/7x0RGREr1ftrRcT7ImLDvtdsUpev2rd8vtdLkiRNN9MuDPY5FdgOuL3eXwt4L7Bh3/M2qctXRZIkqUNmtl2AqZSZ/wD+0XY5JEmShtW0rhlsNvPWpuHf14fOqsszInYETq7Lr6rLrl7Aey4XEQdFxDURcVdEXBQRz5zKzyFJkjRVpnUY7HMd8JL6/30ozcfbAb8G9qvL96zLnrOA9zkR2Bv4CLA7cD5wUkRsMflFliRJmlrTupm4KTPviojf1buXZOZ5vcci4o/1v7/JzKvHe4+IeCqwK7BjZp5dF58WEZsA7wKeP/kllyRJmjpdqhmcDE8DrgfOjYiZvRtwJrD1eC+KiFkRcUFvlLMkSdKw6EzN4CRZA3gA8N8xHpsz3osy8wjgCICIyKkpmiRJ0sQZBifmJuBa4NltF0SSJGkydC0M3l3/XW4Rl/c7E3gLcFtmXjaZBZMkSWpD18LgX4E7gJdHxC3AfzPzAqA3gOQ1EXE8cHtm/n6M158O/Ag4PSIOBC4GVgG2AJbLzHdO+SeQJEmaRJ0aQJKZdwKvBrYCzqZMC0Nm/oUyvcyewLnMm3ew//VZn3Mk8CZKMPwCZTqac6a4+JIkSZMuSr7RoDiAZHBGcduOiLaLIEmahjJz3ANMp2oGJUmSND/DoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpw2a2XQBpqkRE20WYsMxsuwgTNorrWZI0jzWDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYXICIyIjYdxGe976I+OcgyiRJkjSZZrZdgCG3HXBV24WQJEmaKobBBcjM8xb0eEQsDcwdUHEkSZImXaebiSNi34i4JiJmR8R3I+KptWl4x/r4fM3EEfGTiDgxImZFxBXAncADWyq+JEnSEutszWBEPAf4DPA54HvAE4AvL8JLtwceCrwduB24ZarKKEmSNNU6GwaB/YHvZ+Y+9f5pEbEG8LqFvG5VYIvMvKG3ICIW+IKImAXMWoKySpIkTYlONhNHxExgS+Ckvof674/lwmYQXBSZeURmbp2ZW0/kdZIkSVOtk2EQWAOYAfyjb3n//bFMKAhKkiQNs66GwX8Cc4A1+5b33x9LTn5xJEmS2tHJMJiZ9wC/Afboe+hZLRRHkiSpNV0eQPJR4FsRcRilr+D2wK71MecOlCRJndDJmkGAzPw28Abg2cB3gW2A/erDt7ZVLkmSpEGKTLvA9UTEAcC7gNUz844p+huucI1rFH+PC5taSZLUvswcd2fd2WbiiFgTeCdwFmXy6B0oE0l/eaqCoCRJ0rDpbBgE7gY2A14G3A+4Dvg08O42CyVJkjRINhMPmM3EWpBR/D3aTCxJw29BzcSdHUAiSZIkw6AkSVKnGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6bGbbBZA0zyhe53cUr6cMo7muJWkqWDMoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYSMZBiNi74jIiFip3t+w3t9tEt57x/pej1jykkqSJA23mW0XYJJcB2wHXNZ2QSRJkkbJtAiDmXkXcF7b5ZAkSRo1Q91MHBFPjIizIuK2iLglIn4SEVuO8bz7NBNHxNURcXBEvCMirquv/0QUz4yIiyPiPxHx3YhYbYw//8CIOCUiZkfEXyPitX1/8+ER8cOIuKk+59KI2GcKVoMkSdKUGdqawYjYETgdOAt4OTAb2B5YdwJv80LgV8ArgK2AD1EC8BOBdwPLA4cBHwVe2/faLwNfAz4DPAf4fET8LTNPqY+fDFwKvBS4C9gUWGUin1GSJKltkZltl2FMEfELYGlgm+wrZETsDRwFrJyZt0XEhsBVwO69sBYRVwP3AJtm5py67FfAY4CNM/Oquuwg4OWZuXa9vyMlgH4xM2c1/ubp9e89LiLWAP4BPCozf78In2UW0HuvrSa8MqQhNqz7kIWJiLaLIEkDk5nj7vSGspk4IlYEtgW+0h8EJ+gnvSBYXQ5c3QuCjWVrRsQyfa/9Tt/9bwNbRcQM4CbgGuDwiNgrItZaUCEy84jM3Dozt168jyFJkjQ1hjIMAqsBQRklvCT+3Xf/7nGWBdAfBm8c4/5MYI3MnAvsDFwPHAlcHxE/G6s/oyRJ0jAb1jB4MzAXWKfFMvTX9q1FaXb+J0BmXpaZzwVWBZ4GLAecGhHDuk4lSZLuYyiDS2bOBn4JvCza69jznDHuX9jX7Exm/jczfwx8khJeVx1Q+SRJkpbY0I4mBt4BnAH8ICKOoIwm3g64YEB/f5eI+DBwNrAnsBOwB0BEPAo4GDgBuJLSrP124KLMvGlA5ZMkSVpiQ1kzCJCZP6UEsBWAYyjB60nA3wZUhFdRRh5/F9gN2CczT6qPXQ/cALwL+AHwOco0M88aUNkkSZImxdBOLTNdRYQrXNPKqO5DnFpGUpeM3NQykiRJGgzDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR12My2CyBptEVE20VYLJtvvl3bRZiwSy/9RdtF6Iz73/+BbRdhwj717ePbLsKEvexJT2y7CMKaQUmSpE4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDC5ERBwdERe0XQ5JkqSpMLPtAoyADwLLt10ISZKkqWAYXIjMvKLtMkiSJE0Vm4kXotlMHBF7R0RGxCMj4vSImB0Rl0XEnm2XU5IkaXEYBhfPscBJwHOAPwPHR8R67RZJkiRp4mwmXjyfyswjASLiQuAGYDfg8FZLJUmSNEGGwcVzWu8/mfmviLgRGLdmMCJmAbMGUTBJkqSJMAwunn/33b8bWG68J2fmEcARABGRU1guSZKkCbHPoCRJUocZBiVJkjrMMChJktRhhkFJkqQOcwDJQmTm3o3/Hw0cPcZzNhxYgSRJkiaRNYOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHVYZGbbZeiUiHCFS+qMUT3GRETbRZAmVWaOu1FbMyhJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDiyEitouIkyLiuoiYHRG/jYiXtF0uSZKkiZrZdgFG1AbAucDhwJ3A9sBRETE3M49rtWSSJEkTEJnZdhlGWkQEMAP4LLBxZj5ljOfMAmbVu1sNsHiS1KpRPcaUXbs0fWTmuBu1YXAxRMRqwPuBPYB1KWEQ4NrMXG8hr3WFS+qMUT3GGAY13SwoDNpMvHiOBh4HfBC4BLgVeB0lHEqSJI0Mw+AERcRywG7APpl5eGO5g3EkSdLIMcBM3LKU9XZXb0FErAw8q7USSZIkLSZrBicoM2+JiPOB90TErcBc4B3ALcAqrRZOkiRpghxAshgiYiPgC5R+g/8CDgNWAPbNzDUW8lpXuKTOGNVjjANINN04mniIGAYldcmoHmMMg5puFhQG7TMoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6bNLCYERkROw7We/XeN+jI+KCyX5fSZIkWTMoSZLUaYZBSZKkDlukMNhrqo2IZ0fEZRFxZ0ScExEPW8Brdo2I0yPixoi4NSLOi4idG48/rDYt79j3upUi4raIeGPf8p0i4ncRMbv+7Yf3Pb5CRBwaEdfX8p3f/Hv1OVdHxMF9y/au5Vip3l86Ig6OiL9GxF0R8feI+E5ELNN4zfoRcXxE3BQRt0fEjyJi00VZl5IkScNkIjWDGwCfBD4IvBi4H/CjiFhunOc/GDgZ+H/Ac4GfAz+IiO0BMvMS4Dxg777XPR9YGjimsWx94OPAh4EXAWsBJ0RENJ7zReAV9TnPAa4BTo2IJ0zgMwK8E3gJ8G5gJ+BNwC3ADICIWB04B9gUeC3wAmBF4IyIWH6Cf0uSJKldmbnQG3A0kMDjG8s2AO4BXlvvJ7DvOK9fCpgJ/Ag4srH8VcBtwEqNZT8FTuz72/cAGzeWPbv+vc3q/c2BucDL+/7mH4AfNZZdDRzcV7a963utVO+fAnxiAevig8C/gNUby1ajBMZ9xnnNLOCCektv3rx568ptVLW93rx5m+xbLiDnTaRm8MbM/HnvTmb+BbgQeOxYT46I9SLiKxFxLSXM/RfYGdik8bQT6r/Pr695KPAE4Ki+t7s6M//cuH9J/Xe9+u82QADfbJRvbr0/0ZrB3wJ7R8TbIuJRfbWPAE8DTgdujYiZETET+A9lXWw91htm5hGZuXVmjvm4JElSWyYUBsdZtk7/wohYCjgJeDzwHuDJlMD2A+DeZuXM/A/wDUrzLpRauuuBH/a95b/77t9d/+291zrAbZl5e9/zbgBWiIhlx/tQY/gQ8Fng9cBFwDV9/RfXAPaihNvm7cnAgybwdyRJklo3cwLPXWucZRePsXwjYEtgl8y8N9iN06fuS8A5EbEx8DLgq5k5ZwLlArgOWCkiVugLhGsDt2fmXfX+ncAyfa9drXknM++kBNj31DK9FjgkIv5YP8tNlKD7wTHK8Z8JlluSJKlVE6kZXCsiHt+7ExHrA48BfjXGc3uh767G8zcAtu9/Ym16/iNwJGWgyNETKFPP+ZQ28ec1/l7U++c0nvc3Sv/Cpp0ZR22a3o/yOXojp88EHg5cnJkX9N3+uBhllyRJas1Eagb/CRwTEQcAdwDvpzQTHz3Gcy+jBK9PRMS7gZXr868d572/TBkt/IvMvGwCZQIgMy+NiOOAwyJiZeAK4NXAZsDrGk/9DvCZiNifEiCfSwl294qI71D6//2mfs7nUdbTT+tTPgm8FPhxRHymfqa1gScB52TmcRMtvyRJUlsmUjP4F0ot2fuA4ylNok+vzarzqc2ye1IGjpxIaVL9KHD2OO/93frvkRMoT79XA1+hNPF+jzLaebfMbNYMHgEcAryB0lfxLkofwaafU0YrH1vfZyvguZl5Qf1s/wQeRwm8nwJOAw6iTLXzuyUovyRJ0sBFGUG/kCdFHA08YqpGw0bE6ymB6oGZeetU/I1hERELX+GSNE0syjFmGN13IglptGXmuBv1RJqJJ11EbEiZamZ/4OjpHgQlSZKGTdvXJn4fZZLnSylX/JAkSdIALVIzsSaPzcSSumRUjzE2E2u6WVAzcds1g5IkSWqRYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdNrPtAkiSpq9RvcbvBVde2XYRJuzZOzyj7SJM2HXXj956Bpgz5562izCprBmUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMOgJElShxkGJUmSOswwKEmS1GGGQUmSpA4zDEqSJHWYYVCSJKnDDIOSJEkdZhiUJEnqMMPgAkTEbhGREbFh22WRJEmaCoZBSZKkDjMMSpIkddhIhcGIODoiLoiIXSPikoi4PSJOjYjVI2KjiDgrImbX5zyq8bq3RMT5EXFLRNwQESdHxEZ97x0R8b6IuDEi/hMRXwVWGaMMy0XEQRFxTUTcFREXRcQzB/DxJUmSJt1IhcFqfeADwAHALODxwBHA8fX2PGAmcHxERH3NesBhwB7Aq4EZwM8j4n6N930D8J76Xs8D7gAOGuPvnwjsDXwE2B04HzgpIraYtE8oSZI0IDPbLsBiWB3YLjOvAKg1gG8FXp6ZX63LAjgV2Ay4NDPf3HtxRMwATgdupITDr9Zlbwe+kJkH1Kf+KCJOB9ZtvPapwK7Ajpl5dl18WkRsArwLeP5YBY6IWZTgKkmSNFRGsWbw6l4QrC6v//54jGXrAkTE4yLi9Ij4F3APcDuwErBJfd6DgHWA7/X9rW/33X8acD1wbkTM7N2AM4GtxytwZh6RmVtn5rjPkSRJasMo1gz+u+/+3WMs7y1bLiLWB04DfgW8Bvh7ffxUYLn6vAfUf2/se+/++2vU5/53jHLNWZTCS5IkDZNRDIMT9QxgBWCPzJwNUGvzVm885/r671p9r+2/fxNwLfDsKSinJEnSwHUhDC4PzKU0D/e8gPk/+zWUQLgH8MPG8j373utM4C3AbZl52eQXVZIkabC6EAZ/TBk9fFREfBl4OLAfjWblzJwTEQcBB0fEP4GfAc8FNu97r9OBHwGnR8SBwMWU6We2AJbLzHdO9YeRJEmaTKM4gGRCMvP3lKlgtgVOAV5MGfV7S99TD6FMF/Na4FuUASZv63uvpNQWHgm8iRIMvwBsB5wzVZ9BkiRpqkTJNxqUiHCFS9KQu+DKK9suwoQ9e4dntF2ECbvu+tFbzwBz5tyz8CcNmcyM8R6b9jWDkiRJGp9hUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYYZBSZKkDjMMSpIkdZhhUJIkqcMMg5IkSR1mGJQkSeoww6AkSVKHGQYlSZI6zDAoSZLUYZGZbZehUyLCFS5JmnSjeDyPiLaL0BmZOe7KtmZQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFQkiSpwwyDkiRJHWYYlCRJ6jDDoCRJUocZBiVJkjrMMChJktRhhkFJkqQOMwxKkiR1mGFwEkTEHhFxaUTcHRFXt10eSZKkRRWZ2XYZRlpEzABuAn4AHAbMzszfLOD5rnBJ0qQbxeN5RLRdhM7IzHFX9sxBFmSaWgdYBTg2M89puzCSJEkTYTPxIoiIF0TE7yPiroi4JiI+HBEzI2Jv4Jr6tO9FREbE+9orqSRJ0sQYBhciInYGTgB+DewBfAbYj9IkfCqwZ33qfsB2wJdaKKYkSdJisc/gQkTEecAdmfnkxrK3AR8FNqA0tV8F7J6ZpyzC+7nCJUmTbhSP5/YZHJwF9Rm0ZnAB6uCQxwDf7HvoBMq6224R32dWRFwQERdMchElSZKWiANIFmwNYGngrSb4EQAAC0xJREFUhr7lvfurL8qbZOYRwBFgzaAkSfr/7d1rrGX1Wcfx3wNUKF5aEZnehFqJ2lSSQgeiaUtLSQNJm9RLtFqMXApjm0C0QRvbF9ZKjERDMfaSdOwFSLUi0WovJgOBtiAUEkoxaE0ADcQiggVagtzh8cVeA9uTmeHSc/Y+m//nk0z2nLX/e51nrxfs76y1zmFzcWZwz76d5JEkB63ZvmV6vHux4wAArC8xuAfd/ViSryf55TVP/UqSx5N8beFDAQCsI5eJn9oHkuyoqk8n+eskhyU5K8lfdPe3qurlS5wNAOB74szgU+jui5P8apKtSb6Q5LeTnJPk9GXOBQCwHvxqmQXzAyQAbIRV/Dz3q2UWx6+WAQBgl8QgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDE4DqoqtOrqpc9BwDAMyUGAQAGJgYBAAa2kjFYVcdUVVfVS+a2fa2qHquqF85tu6Gq/mj6+6ur6tKqur+q7qmqv6yqLWv2e2BVnV9Vd03rvlJVW9es2beqPlJV36mqu6vq3CTP2+C3DACwIVYyBpNck+SRJK9PkqraP8lrkjyc5LXTtgOSvCrJFVX1o0m+kmT/JO9IckaSNyS5pKq+b26/f5/kuCS/k+TtmR2fL1fVoXNrzk5yapKzkpyQ5JAkZ27EmwQA2Gj7LHuAZ6O776+qr2cWgxcm+dkk301y6bTtS0lel6STXJXk/dNLj+vue5Okqm5KcnWSX0ry2ao6PrOQfGN3f3Vac1mSW5L8bpLfrKofSfKuJB/o7nOmNTuSfHNP81bVtiTb1uXNAwCso1U9M5gkl2c6M5jk6CT/lOSra7b98xR/RyW5eGcIJkl3X5NZ6L1u2nRUkjt3huC05n+TfHFuzWFJ9kvyD3NrHp//ele6e3t3b+3urXtaBwCwaKscg1ck+ZnpHsHXT19fkWRrVe03ty1JXpzkjl3s444kB8ytufMp1rxoely7blevAwDY9FY5Bq+cHt+Y2WXiy5P8a5L7khyb5Ig8GYO3JzloF/vYkuTuZ7Dmv6fHtet29ToAgE1vZWOwu+9J8i9J3pPksSTf6O7O7HLxezO7H3JnDF6T5Liq+sGdr6+qI5O8fFq/c81BVXX03Jr9k7xlbs0NSR5M8ra5NXvNfw0AsEpWNgYnV2R2b+BV3f3Ymm03dffOS8Mfmh53VNXbquqEJH+XWdz9bZJ0947Mftjkwqo6saremuQfkzw/yZ9Oa+5Ksj3JB6vqzOmHTi5K8gMb/D4BADbEcyEGk9kl4rXbdp7NS3f/T5JjMjur99kkH53Wvbm7H5577c8nuSTJn2UWeZXkTd1989ya9yb5VJLfn/b1X3kyNgEAVkrNrqyyKP4fxgBshFX8PK+qZY8wjO7e7cFe9TODAAB8D8QgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAqruXPcNQqsoBh02gavX+Ldz9+LJHYBM75V0fXPYIz9gD9z247BGelauv/NKyR3hGbrvtpjz00P21u+dX77+GAACsGzEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwsH2WPcAIqmpbkm3LngMAYC0xuADdvT3J9iSpql7yOAAAT3CZGABgYGIQAGBgYhAAYGBicJ1U1W9U1aNVdciyZwEAeLrE4PrZK8neSWrZgwAAPF1icJ1093ndXd19y7JnAQB4usQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwMQgAMDAxCAAwMDEIADAwKq7lz3DUKrKAQdg3a3i53lVLXuEYXT3bg+2M4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADW7kYrKqfWML3fFFV7b/o7wsAsNFWIgarar+qOqGqLkty09z2varq96rq5qp6qKpurKoTd/H606vqpmnNzVX1njXPv6yq/qaq7qyqB6rq36vqrLklxye5vao+XlVHbtgbBQBYsH2WPcCeVNXhSd6Z5IQk+yf5fJK3zC35cJITk/xhkuuSvDnJp6rqru7+4rSP06Z1H0qyI8kxSc6pqn27++xpPxckeX6SbUm+k+QVSX567vt8LskPJTk5ybaquiHJJ5J8prvvXu/3DQCwKNXdy57h/6mqF2QWf+9MckSS65N8OmvCq6oOTXJjkpO7+/y57RckeWV3H1lVeyX5zyQXd/fJc2s+Nn2PLd39YFXdl+TXuvsLT2O+IzKLwnck+f7MQvGTSS7tp3Ewq2pzHXAAnhM22+f501FVyx5hGN2924O9qS4TV9XxSW5PclaSK5Mc3t2Hd/ef7+IM3LFJHk/yuaraZ+efJJcmeXVV7Z3kZUlekuSiNa+9MLMzfYdNX1+f5I+r6qSqOnhPM3b3dd19xrTfE5P8cGZnHP9jD+9rW1VdW1XXPtUxAABYpE0Vg0keSnJ/kv2SvCDJC2v3/2w4MMneSb6b5JG5P+dldvn7xdOfJLljzWt3fn3A9Pj2JNcmOTfJrVV1fVUd+xSzPjFjZsfxnt0t7O7t3b21u7c+xT4BABZqU90z2N1frqqXJvmFJKcmuSzJLVV1XpLzu/vWueV3J3k0yWszO0O41p15MnYPWvPclrl9pLtvS3LSdFn5qCR/kOTzVXVwd9+180VTmL4ps8vEv5jk4SR/leTd3f2NZ/OeAQCWadPdMzivqn48ySlJTkry0szi8Lzu/kxV/VSSf0tyXHdfspvX77xncEd3nzK3/aNJfj3TPYO7eN3PJbkqyWu6+7qq2pLk3dMchyS5PLP7BC/q7gee4XvavAccgJW1mT/Pd8c9g4uzp3sGN3UM7jTd/3d8ZmcL39rdz5u2fyyzS7x/ktll3v2SvCrJT3b3qdOa05J8PMk5SS5J8oYk70vy/u4+e/qBlR2Z/UTxjUn2TXJmklcmeUV3P1BVJyU5O8n5ST7R3U/8eptn8V42/wEHYOWswuf5WmJwcVY+BudV1ZbuvmP6eyX5rSSnJTk0yb1Jvpnkk919wdxrzpjWHZzkW0k+3N3nTs/tm+QjSY5O8mOZ3bN4dZL3dfcN05oDktzb3Y+uw/yrdcABWAmr9nmeiMFFek7F4KoTgwBshFX8PBeDi7Myv1oGAIDFEoMAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAAxODAAADE4MAAAMTgwAAA9tn2QMM6NtJbt2A/R447XvVrOLcZl6cVZzbzIuxijMnGzh3VW3EbpPVPNarOHOycXMfsqcnq7s34HuyaFV1bXdvXfYcz9Qqzm3mxVnFuc28GKs4c7Kac5t5cZY1t8vEAAADE4MAAAMTg88d25c9wLO0inObeXFWcW4zL8Yqzpys5txmXpylzO2eQQCAgTkzCAAwMDEIADAwMQgAMDAxCAAwMDEIADCw/wMmSRXa4xiaLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72SZE9aauZwQ"
      },
      "source": [
        "### Observations\r\n",
        "\r\n",
        "1.   Inspite of having more parameters than the RNN model, this model trains faster due to computations performed in parallel\r\n",
        "2.   The performance of CNN model is better than the RNN model.\r\n",
        "\r\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8XZJXLpfGq"
      },
      "source": [
        "Get data and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tOVs0Zopdfg",
        "outputId": "e68b6fb4-a545-4a6f-96d6-035a7eda887f"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# get length of sentence to help RNN learn only the unpadded sentences\n",
        "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
        "# this dtype converted to float coz criterion needs data to be in floatTensor. Originally it is LongTensor\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "print(TEXT)\n"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.data.field.Field object at 0x7fd9b47129e8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rvx7gvcr4mN",
        "outputId": "506578b4-47a3-4ce3-f482-4cdb384a1f38"
      },
      "source": [
        "import random\n",
        "# Get the train and test data splits using TEXT and LABEL criteria\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "train_data,valid_data=train_data.split(random_state = random.seed(SEED))\n",
        "print(len(train_data))\n"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvOcv1l7I7bM"
      },
      "source": [
        "Interesting aspect to explore is if the model works when trained on reversed string input, given we use GloVe word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HqezcS1OnhT"
      },
      "source": [
        "'''train on reversed string.'''\n",
        "for sentence in train_data:\n",
        "  vars(sentence).get('text').reverse()"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxRAGDCzfD62"
      },
      "source": [
        "# validate on not reversed and reversed string\n",
        "for sentence in valid_data:\n",
        "  vars(sentence).get('text').reverse()\n",
        "\n"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONAXuvG4P7L7",
        "outputId": "966dfe93-0734-4693-a908-93e5ca97b7c6"
      },
      "source": [
        "for i in train_data[0:7]:\n",
        "  print(vars(i)['text'])\n",
        "  # break\n",
        "print('-----')\n",
        "for i in valid_data[0:7]:\n",
        "  print(vars(i)['text'])"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.', 'eight', 'an', 'maybe', 'as', 'high', 'as', ',', 'rating', 'higher', 'a', 'this', 'given', 'have', 'might', 'I', 'and', 'funnier', 'be', 'might', 'he', 'mental', 'so', 'act', \"n't\", 'did', 'just', 'Dane', 'If', '.', 'joke', 'Pieces', \"'s\", 'Reese', 'the', 'liked', 'also', 'I', '.', 'thru', 'drive', 'the', 'at', 'yell', 'people', 'where', 'joke', 'King', 'Burger', 'the', 'like', 'jokes', 'good', 'few', 'a', 'had', 'He', '.', 'alive', 'was', 'he', 'when', 'that', 'into', 'really', 'was', 'Dangerfield', 'Rodney', 'sure', \"'m\", 'I', '.', 'star', 'rock', 'a', 'be', 'to', 'wants', 'comedian', 'every', 'that', 'said', 'He', '?', 'something', 'or', 'stage', 'on', 'up', 'both', 'there', 'because', 'Just', '.', 'stars', 'rock', 'and', 'comics', 'toward', 'comparison', 'his', 'understand', 'quite', \"n't\", 'did', 'I', '.', 'laugh', 'us', 'make', 'and', 'together', 'yourself', 'get', ',', 'down', 'Calm', '.', 'much', 'To', '.', 'floor', 'the', 'on', 'rolling', 'and', 'himself', 'on', 'water', 'spilling', ',', 'crazy', 'going', 'and', 'yelling', 'his', 'of', 'all', 'with', 'extreme', 'to', 'is', 'and', 'hard', 'to', 'tries', 'Dane']\n",
            "['.', 'Lewis', 'Juliette', 'by', 'performance', 'wonderful', 'that', 'and', ',', 'dialogue', 'great', ',', 'cinematography', 'good', ',', 'half', 'second', 'the', 'in', 'especially', ',', 'scenery', 'great', ',', 'concept', 'clever', 'a', 'to', 'thanks', ',', 'film', 'entertaining', 'an', 'is', '\"', 'Kalifornia', '\"', ',', 'however', ',', '/>Overall', '/><br', 'disappointing.<br', 'somewhat', 'was', 'that', 'ending', 'an', ',', 'violence', 'preposterous', 'and', 'unnecessary', 'into', 'messily', 'rather', 'dissolves', 'plot', 'The', '.', 'mannequins', 'test', 'of', 'full', 'house', 'old', 'dilapidated', 'a', 'with', 'site', 'test', 'nuclear', 'Nevada', 'a', 'enter', 'travelers', 'our', 'as', ',', 'it', 'to', 'feel', 'Zone', 'Twilight', 'a', 'on', 'takes', 'film', 'the', ',', 'end', 'the', '/>Toward', '/><br', 'Adele.<br', 'like', '-', 'child', ',', 'animated', 'highly', ',', 'naive', 'the', 'as', ',', 'stunning', 'is', 'she', ',', 'accent', 'Southern', 'duty', '-', 'heavy', 'and', 'voice', 'nasal', 'her', 'With', '.', 'Lewis', 'Juliette', 'to', 'goes', 'performance', 'best', 'for', 'choice', 'my', 'But', '.', 'artist', 'photographic', ',', 'garde', '-', 'avant', 'the', 'as', 'great', 'is', 'Forbes', 'Michelle', '.', 'times', 'at', 'overacting', 'his', 'despite', ',', 'effective', 'surprisingly', 'is', 'Pitt', 'Brad', '.', 'flat', 'is', 'performance', \"'s\", 'Duchovny', '.', 'uneven', 'is', 'acting', 'the', ',', '\"', 'Kalifornia', '\"', '/>In', '/><br', 'unstoppable.<br', 'so', 'seemed', 'terror', 'whose', ',', 'hitchhiker', 'maniacal', \"'s\", 'everyone', ',', 'Ryder', 'John', 'like', 'more', 'and', 'more', 'act', 'to', 'starts', 'Early', 'and', ',', ')', '1986', '(', '\"', 'Hitcher', 'The', '\"', 'like', 'more', 'and', 'more', 'look', 'to', 'starts', '\"', 'Kalifornia', '\"', ',', 'landscape', 'stark', 'beautifully', 'its', 'with', ',', 'Southwest', 'desert', 'the', 'enter', 'they', 'As', 'U.S.', 'the', 'across', 'west', 'proceed', 'travelers', 'four', 'the', 'as', 'Carrie', 'and', 'Brian', 'to', 'obvious', 'more', 'ever', 'become', 'which', ',', 'problems', 'mental', ',', 'well', ',', 'some', 'has', 'Early', 'poor', '/>But', '/><br', 'law\".<br', 'state', ',', 'free', 'is', 'rent', \"'s\", 'month', 'first', 'your', 'hear', 'I', 'and', ',', 'limits', 'speed', 'no', 'got', \"n't\", 'ai', 'they', 'and', '...', 'trees', 'the', 'on', 'all', \"'s\", 'it', 'account', 'on', ',', 'fruit', 'no', 'buy', 'to', 'have', 'never', 'You', '\"', ':', 'California', 'about', 'Adele', 'instruct', 'to', 'continues', 'Early', '.', '\"', 'does', 'sure', 'It', '\"', ':', 'proudly', 'responds', 'Early', 'which', 'To', '.', '\"', 'here', 'around', 'people', 'stupid', 'many', 'so', 'are', 'there', 'why', 'explains', 'that', 'guess', 'I', '\"', ':', 'Adele', 'convince', 'to', 'enough', \"'s\", 'That', '.', '\"', 'stupid', 'people', 'makes', 'weather', 'cold', ';', 'weather', 'warm', 'that', 'all', 'of', 'account', 'on', ',', 'there', 'out', 'faster', 'think', 'People', '\"', ':', 'California', 'of', 'idea', \"'s\", 'Early', '.', '\"', 'Hillbillies', 'Beverly', 'The', '\"', 'of', 'reruns', 'since', 'seen', \"n't\", 'have', 'I', 'ways', 'in', 'behave', 'otherwise', 'and', ',', 'drool', ',', 'speculate', ',', 'muse', ',', 'confide', ',', 'cackle', ',', 'babble', 'they', 'as', ',', 'entertaining', 'so', 'film', 'this', 'make', 'what', 'are', 'Adele', 'and', 'Early', ',', '/>Indeed', '/><br', 'cinema.<br', 'of', 'all', 'in', 'find', 'never', 'will', 'you', '\"', 'trash', 'white', 'poor', '\"', 'of', 'examples', 'better', 'two', ',', ')', 'Lewis', 'Juliette', '(', 'Adele', ',', 'girlfriend', 'his', 'and', ')', 'Pitt', 'Brad', '(', 'Grayce', 'Early', ',', 'couple', 'young', 'a', 'is', 'with', 'up', 'end', 'they', 'Who', '.', 'expenses', 'travel', 'share', 'to', 'someone', 'for', 'advertise', 'they', 'so', ',', 'sites', 'murder', 'of', 'tour', 'country', '-', 'cross', 'a', 'for', 'money', 'the', 'has', ',', ')', 'Forbes', 'Michelle', '(', 'Carrie', ',', 'girlfriend', 'his', 'nor', ',', 'he', 'neither', ',', 'But', '.', 'killers', 'serial', 'American', 'on', 'research', 'field', 'conduct', 'to', 'wants', ',', 'writer', 'urbane', ',', 'sophisticated', 'a', ',', ')', 'Duchovny', 'David', '(', 'Kessler', 'Brian', '.', 'board', '-', 'on', 'killer', 'a', 'with', ',', 'movie', 'road', 'a', \"'s\", 'It']\n",
            "['.', 'them', 'thank', 'we', 'and', ',', 'return', 'we', ',', 'watch', 'We', '.', 'parts', \"'s\", 'people', 'of', 'lot', 'a', 'on', 'effort', 'huge', 'a', \"'s\", 'it', '\\x85 ', 'graphics', ',', 'costumes', ',', 'sets', ',', 'People', '.', 'magnitude', 'this', 'of', 'production', 'a', 'on', 'put', 'to', 'effort', 'of', 'lot', 'a', 'takes', 'It', '.', 'due', 'is', 'credit', 'where', 'credit', 'involved', 'people', 'the', 'give', 'to', 'have', 'you', 'but', '\\x85 ', 'word', 'the', 'of', 'sense', 'every', 'in', 'pleasure', 'guilty', 'total', 'a', 'is', 'Frontier', '/>Hidden', '/><br', 'jerkers.<br', '-', 'tear', 'real', 'actually', 'are', 'them', 'of', 'some', '\\x85 ', 'wise', '-', 'story', 'out', 'stands', 'really', 'two', 'or', 'show', 'a', ',', 'sometimes', 'And', '.', 'seen', \"'ve\", 'you', 'anything', 'almost', 'than', 'Better', '.', 'rate', 'first', 'are', 'graphics', 'The', '?', 'Why', '.', 'that', 'by', 'get', 'can', 'We', '.', 'Okay', '.', 'show', 'every', 'of', 'end', 'and', 'beginning', 'the', 'at', 'played', 'Quest', 'Galaxy', 'from', 'theme', 'the', 'hear', 'you', 'when', 'or', ',', 'movies', 'the', 'and', 'shows', 'other', ',', 'TNG', ',', 'TOS', ':', 'ST', 'from', 'quote', 'clearly', 'they', 'when', 'bit', 'a', 'cringe', 'you', ',', 'Yes', ')', '.', 'on', 'and', 'on', 'go', 'to', 'seem', 'scenes', 'kissing', 'the', 'of', 'some', 'although', '(', 'kiss', 'males', 'Starfleet', 'two', 'watching', ')', 'too', 'cool', 'little', 'a', 'admittedly', 'though', '(', 'odd', 'little', 'a', \"'s\", 'it', ',', 'Yes', '.', 'hokey', 'is', 'dialogue', 'the', 'of', 'some', ',', 'Yes', '.', 'watch', 'to', 'fun', 'actually', \"'s\", 'it', ',', 'together', 'put', 'have', 'neighbors', '&', 'friends', 'these', 'everything', 'for', ',', 'admit', 'to', 'have', 'you', ',', 'Still', '.', 'saying', \"'re\", 'they', 'that', 'word', 'solitary', 'single', 'a', 'understand', 'to', 'impossible', 'virtually', 'it', 'makes', 'whick', ',', 'place', 'first', 'the', 'in', 'well', 'speak', 'to', 'seem', \"n't\", 'do', 'simply', 'or', ',', 'accents', 'thick', 'have', 'them', 'of', 'some', '\\x85 ', 'marbles', 'on', 'walking', \"'re\", 'they', 'like', 'lines', 'their', 'through', 'stumble', 'them', 'of', 'some', '\\x85 ', 'thinnest', 'the', 'or', ',', 'youngest', 'the', 'or', ',', 'seen', 'ever', \"'ve\", 'you', 'folks', 'looking', 'beautiful', 'most', 'the', \"n't\", 'are', 'people', 'these', 'of', 'lot', 'A', '.', 'tell', 'can', 'you', ',', 'to', 'neighbors', '&', 'friends', 'definitely', \"'s\", 'It', '.', 'show', 'a', 'on', 'put', ',', 'it', 'put', 'once', 'Judy', '&', 'Mickey', 'as', ',', 'to', 'machines', 'sewing', 'and', 'cameras', 'video', 'home', 'and', 'programs', 'computer', 'the', 'have', 'who', 'neighbors', '&', 'friends', 'mainly', 'features', 'it', 'and', ',', 'web', 'the', 'on', 'only', 'available', ',', 'series', 'made', 'fan', 'a', \"'s\", 'it', ',', 'First', '.', 'ways', 'many', 'in', 'you', 'surprise', 'will', 'Frontier', 'Hidden', 'Trek', 'Star']\n",
            "['!', 'good', 'Very', '.', 'Stars', 'Greatest', '50', \"'s\", 'TV', 'on', '3', 'number', 'was', 'Thaw', 'John', '.', 'Drama', 'Popular', 'Most', 'for', 'Award', 'Television', 'National', 'the', 'won', 'it', 'and', ',', 'Award', 'Grade', 'Lew', 'BAFTA', 'the', 'nominated', 'was', 'It', '.', 'film', 'excellent', 'an', 'is', 'this', 'War', 'World', 'Second', 'the', 'during', 'Set', '.', ')', 'Dad', 'or', '(', 'family', 'loving', 'a', 'with', 'home', 'his', 'gets', 'Willaim', 'end', 'the', 'In', '.', 'him', 'find', 'to', 'London', 'to', 'goes', 'he', 'him', 'contacting', 'not', 'William', 'about', 'worried', 'gets', 'Tom', 'After', '.', 'back', 'him', 'wants', ')', 'Apsion', 'Annabelle', '(', 'Mum', \"'s\", 'Willaim', 'Until', '.', 'friendship', 'a', 'develop', 'to', 'starts', 'he', 'child', 'this', 'know', 'to', 'gets', 'he', 'As', '.', ')', 'Robinson', 'Nick', '(', 'Beech', 'Willaim', 'called', 'evacuee', 'an', 'with', 'landed', 'been', 'has', 'he', 'now', 'and', ',', 'died', 'son', 'and', 'wife', 'his', 'since', 'while', 'a', 'for', 'alone', 'village', 'a', 'in', 'lived', 'has', 'man', 'widowed', ')', 'Thaw', '(', 'Oakley', 'Tom', '.', ')', 'Morse', 'Inspector', 'and', 'Q.C.', 'Kavanagh', '(', 'Thaw', 'John', 'winning', 'BAFTA', 'by', 'role', 'favourite', 'and', 'best', 'the', 'probably', 'is', 'This', '.', 'one', 'brilliant', 'a', 'is', 'film', 'TV', '-', 'for', '-', 'made', 'This']\n",
            "['.', 'production', 'theatrical', 'school', 'high', 'best', 'the', 'of', 'worthy', 'accents', '\"', 'Spanish', '\"', 'embarrassing', 'with', 'dialogue', 'done', '-', 'badly', 'of', 'mishmash', 'a', 'delivering', ',', 'quality', 'inferior', 'of', 'is', 'cast', 'supporting', 'the', 'and', ')', 'film', 'the', 'in', 'find', 'could', 'I', 'points', 'high', 'only', 'the', '(', 'Mira', 'of', 'shots', 'leg', 'good', 'many', 'not', 'are', 'Tbere', '.', 'shelf', 'the', 'on', 'it', 'find', 'you', 'if', 'DVD', 'the', 'from', 'away', 'Step', '!', 'victim', 'next', 'the', 'be', 'not', 'do', '--', 'you', 'warn', 'me', 'let', '/>But', '/><br', 'victims.<br', 'various', \"'s\", 'film', 'the', 'of', 'plight', 'the', 'to', 'is', 'she', '\"', 'sensitive', '\"', 'how', 'indicate', 'to', 'supposed', 'are', 'that', 'stares', 'wistful', 'to', 'mostly', 'confined', ',', 'existent', '-', 'non', 'is', 'acting', 'Her', '.', 'project', 'this', 'to', 'herself', 'undersold', 'have', 'to', 'seems', ',', 'films', 'rated', '-', 'top', 'other', 'and', '\"', 'Aphrodite', 'Mighty', '\"', 'in', 'hot', ',', 'Sorvino', '/>Mira', '/><br', 'scenes.<br', 'the', 'of', 'any', 'for', 'done', 'was', 'take', 'one', 'than', 'more', 'doubt', 'I', ',', 'home', 'and', 'set', 'the', 'off', 'get', 'to', 'anxious', 'were', 'cast', 'the', 'if', 'wondered', 'who', 'reviewer', 'previous', 'the', '/>Like', '/><br', 'off.<br', 'them', 'setting', 'STILL', 'are', 'they', 'and', 'adults', 'are', ')', 'rental', 'video', 'local', 'your', 'of', 'section', 'DVD', 'the', 'in', '\"', 'Death', 'of', 'Angel', '\"', '(', '\"', 'Santa', 'Semana', '\"', 'of', 'cast', 'and', 'directors', ',', 'producers', 'the', 'But', '.', 'then', 'funny', 'considered', 'was', 'It', '.', 'stinkbombs', 'off', 'set', 'occasionally', 'would', 'boys', 'us', 'of', 'some', ',', 'school', 'high', 'junior', 'in', 'were', 'we', 'When']\n",
            "['.', 'movie', 'Disney', 'a', 'like', 'look', '\"', 'Tension', 'Haute', '\"', 'makes', 'This', '.', 'gore', 'extreme', 'extreme', ',', 'yes', 'yes', ',', 'gore', 'Extreme', '.', 'now', 'right', 'anything', 'for', 'it', 'trade', \"n't\", 'would', 'I', 'and', ',', 'unique', 'so', 'this', 'makes', 'what', \"'s\", 'that', 'But', '.', 'place', 'the', 'over', 'all', 'names', 'missing', 'have', 'that', 'credits', 'and', ',', 'inserted', 'be', 'still', 'to', 'scenes', 'few', 'a', ',', 'soundtrack', 'no', 'with', ',', 'first', 'at', 'difficult', 'little', 'a', \"'s\", 'it', ',', 'Yes', '.', 'gorehounds', 'for', 'jewel', 'rare', 'a', '\"', 'Cut', 'Rough', '\"', 'this', 'makes', 'it', ',', 'presentation', 'uncut', 'totally', 'this', 'in', 'released', 'be', 'ever', 'would', 'movie', 'this', 'doubt', 'I', 'though', 'even', ',', 'all', 'of', 'first', ',', 'Well', '.', 'this', 'of', 'all', 'of', 'think', 'to', 'what', ',', '/>So', '/><br', 'scenes.<br', 'gore', 'extreme', 'really', 'other', 'the', 'of', 'most', 'with', 'along', ',', 'IMO', ',', 'DVD', 'proper', 'a', 'on', 'scene', 'this', 'see', 'EVER', 'NEVER', 'will', 'you', ',', 'Obviously', '.', 'hell', 'to', 'all', 'laughing', 'myself', 'found', 'I', 'that', ',', 'top', 'the', 'over', 'so', ',', 'extreme', 'so', 'is', 'This', '.', 'floor', 'bathroom', 'the', 'over', 'all', 'laying', 'out', 'pulled', 'are', 'innards', 'more', 'and', 'more', 'and', 'screaming', 'is', 'guy', 'the', 'as', ',', 'FOREVER', 'on', 'goes', 'This', '.', 'asshole', 'his', 'from', 'right', ',', 'is', 'there', 'else', 'ever', 'hell', 'the', 'what', 'and', ',', 'intestines', ',', 'entrails', 'guys', 'the', 'all', 'out', 'rips', 'and', 'in', 'hands', 'his', 'puts', 'then', 'geek', 'The', '.', 'hole', 'ass', 'the', 'at', 'open', 'cut', 'and', ',', 'geek', 'a', 'from', 'behind', 'from', 'caught', 'is', ')', 'who', 'matter', \"n't\", 'does', '(', 'guy', 'This', '.', 'seen', 'have', 'I', 'scenes', 'gore', 'best', 'the', 'of', 'one', 'is', 'this', ',', 'admit', 'to', 'have', 'I', 'and', ',', 'house', 'the', 'inside', 'happens', 'scene', 'gore', 'best', 'the', '/>But', '/><br', 'huh?<br', 'on', 'turn', 'real', 'a', ',', 'Yummy', '.', 'pot', 'a', 'into', 'drains', 'blood', 'her', 'and', ',', 'glee', 'graphic', 'in', 'out', 'pulled', 'are', 'entrails', 'her', 'all', 'and', ',', 'crotch', 'to', 'neck', 'from', 'cut', 'is', ',', 'all', 'and', 'body', 'beautiful', 'her', ',', 'Jameson', 'Jenna', 'later', '/>Then', '/><br', 'too.<br', 'fit', 'probably', 'will', 'it', 'and', ',', 'word', 'right', 'the', 'find', 'you', ',', 'gruesome', ',', 'gross', ',', 'taste', 'bad', 'very', ',', 'Yes', '.', ')', 'that', 'believe', 'can', 'you', 'if', '(', 'on', '-', 'hard', 'a', 'has', 'torso', 'the', 'and', ',', 'fire', 'a', 'over', 'slowly', 'turning', ',', 'pit', 'barbecue', 'a', 'on', 'torso', 'his', 'see', 'we', 'then', 'and', ',', ')', 'scene', 'a', 'insert', 'to', 'cue', 'a', 'with', ',', 'camera', 'off', '(', 'off', 'limbs', 'his', 'of', 'all', 'cuts', 'then', 'geek', 'This', '.', 'table', 'a', 'to', 'bound', 'and', ',', ')', 'stuff', 'TCM-2', '(', 'location', 'like', 'cave', 'a', 'to', 'taken', 'is', 'killing', 'first', 'the', 'from', 'boyfriend', 'the', 'as', ',', 'behold', 'to', 'something', 'is', 'scene', 'gore', 'next', '/>The', '/><br', 'plot.<br', 'kill', 'and', 'stalk', 'standard', 'your', \"'s\", 'it', 'because', ',', 'story', 'the', 'skip', 'just', 'to', 'going', \"'m\", 'I', '.', 'realistic', 'more', 'much', 'look', 'effects', 'the', 'except', ',', 'folks', 'here', 'territory', 'Lewis', 'Gordon', 'Herschell', 'in', 'are', 'We', '.', 'place', 'the', 'over', 'all', 'limbs', 'severed', 'and', ',', 'blood', ',', 'entrails', 'with', ',', 'GORE', 'EXTREME', 'call', 'would', 'I', 'what', 'is', 'This', '.', 'two', 'in', 'sliced', 'completely', 'been', 'has', 'she', ',', 'up', 'her', 'pulls', 'he', 'when', 'and', 'him', 'of', 'front', 'in', 'hanging', 'shoulders', 'and', ',', 'arms', ',', 'head', 'her', 'is', 'sees', 'he', 'All', '.', 'help', 'for', 'crying', 'her', 'with', ',', 'cliff', 'a', 'from', 'hanging', 'girl', 'his', 'find', 'guy', 'the', 'as', ',', 'scene', 'gore', 'great', 'a', 'is', 'This', '.', 'mutants', 'inbred', 'local', 'the', 'of', 'one', 'by', 'attacked', 'is', 'couple', 'camping', 'a', 'as', ',', 'gore', 'of', 'dose', 'HUGE', 'a', 'with', 'off', 'starts', 'movie', '/>The', '/><br', 'spooky)<br', '(', '.', 'Halloween', 'it', 'call', 'we', 'as', 'or', ',', 'Samhain', 'of', 'celebration', 'the', 'and', 'Druids', 'ancient', 'the', 'involving', ',', 'folklore', 'on', 'heavy', 'is', 'that', 'area', 'An', '.', 'woods', 'the', 'of', 'middle', 'the', 'in', 'home', 'a', 'in', 'staying', 'up', 'end', 'and', 'Ireland', 'in', 'vacation', 'a', 'on', 'tourists', 'American', ',', 'standard', 'pretty', 'is', 'story', 'The', '.', 'spice', 'and', 'sugar', 'little', 'a', 'add', 'to', 'and', ',', 'killing', 'the', 'for', 'there', 'just', 'are', 'they', 'but', ',', 'stars', 'porn', 'few', 'a', 'are', 'there', 'Yes', '.', 'allow', 'would', 'NC-17', 'an', 'even', 'than', 'more', 'much', ',', 'graphic', 'extremely', 'is', 'gore', 'the', 'because', ',', 'Reason', '.', 'see', 'to', 'likely', 'ever', 'are', 'you', '\"', 'Samhain', '\"', 'of', 'version', 'uncut', 'only', 'the', 'BUT', ',', '\"', 'Cut', 'Rough', '\"', 'a', ',', 'says', 'it', 'as', 'exactly', \"'s\", 'It', '.', 'completed', 'is', 'movie', 'the', 'when', 'insertion', 'for', 'cue', 'a', 'as', 'appears', 'message', 'a', ',', 'missing', 'is', 'scene', 'or', 'effect', 'an', 'when', 'and', ',', 'affects', 'sound', 'with', 'audio', 'have', 'does', 'movie', 'the', 'of', 'most', ',', 'However', '.', 'later', 'in', 'looped', 'be', 'to', 'meant', 'are', 'they', 'think', 'I', 'because', ',', 'dialog', 'the', 'on', 'audio', 'no', 'have', 'scenes', 'the', 'of', 'some', 'and', ',', 'soundtrack', 'music', 'NO', 'absolutely', 'is', 'There', '.', 'stops', 'chapter', 'no', 'has', 'and', 'minutes', '90', 'over', 'little', 'a', 'runs', 'movie', '/>The', '/><br', 'SAMHAIN.<br', 'of', '\"', 'print', 'work', '\"', 'the', 'upon', 'based', 'review', 'my', 'is', 'this', ',', 'missed', 'you', 'what', 'know', 'you', 'so', 'Just', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '>', '/', '/><br', 'intended.<br', 'director', 'the', 'what', 'see', 'to', 'get', \"n't\", 'did', 'gorehounds', 'bad', 'too', 'and', ',', 'ever', 'movies', 'horror', 'mishandled', 'and', 'controversial', 'most', 'the', 'of', 'one', 'be', 'may', '\"', 'Samhain', '\"', '.', 'everybody', 'for', 'up', 'it', 'screwed', 'and', ',', 'hands', 'their', 'in', 'classic', 'near', 'a', 'had', 'idiots', 'The', '.', 'produced', 'better', 'be', 'will', 'project', 'next', 'his', 'hopefully', 'and', ',', 'goods', 'the', 'deliver', 'to', 'vision', 'and', 'skill', 'the', 'has', 'definitely', 'Viel', 'Christian', '.', 'library', 'gorehounds', 'any', 'for', 'jewel', 'a', \"'s\", 'it', ',', 'me', 'to', 'because', ',', '\"', 'cut', 'rough', '\"', 'that', 'have', 'I', 'glad', 'just', \"'m\", 'I', '.', 'seen', 'partially', 'be', 'least', 'at', 'can', 'gore', 'extreme', 'the', 'of', 'some', 'and', ',', 'inbreds', 'the', 'and', ',', 'stars', 'porn', 'the', 'has', 'still', 'It', '.', 'years', 'in', 'films', 'gore', 'best', 'and', 'goriest', 'the', 'of', 'one', 'being', 'of', 'potential', 'the', 'had', 'this', 'because', ',', 'shame', 'damn', 'a', 'is', 'This', '.', 'down', 'trimmed', 'highly', 'though', 'even', ',', 'up', 'stand', 'still', 'elements', 'gore', 'basic', 'the', 'because', ',', 'much', 'that', 'himself', 'distant', 'to', 'need', \"n't\", 'does', 'really', 'Christian', '.', 'ending', 'up', 'screwed', 'new', 'completely', 'a', 'with', 'edit', 'down', 'trimmed', 'rated', '-', 'R', 'this', 'with', 'do', 'to', 'nothing', 'had', 'he', 'says', 'he', 'and', ',', '.', 'minutes', '79', ',', 'version', 'down', 'cut', 'this', 'for', 'director', 'the', 'from', 'apology', 'an', 'see', \"'ll\", 'you', ',', 'board', 'message', 'the', 'check', 'you', 'If', '.', 'have', 'I', 'version', '\"', 'cut', 'rough', '\"', 'the', 'like', 'nothing', \"'s\", 'it', ',', 'it', 'like', 'still', 'I', 'though', 'even', 'and', ',', '\"', 'Samhain', '\"', 'of', 'version', 'this', 'watching', 'through', 'got', 'Just']\n",
            "['.', 'ever', 'than', 'more', 'target', 'the', 'missing', 'up', 'ends', 'but', 'comeback', 'thriller', 'a', 'making', 'tries', 'Keaton', 'Michael', '.', 'watch', 'to', 'nothing', 'is', 'movie', 'this', ',', 'edited', 'horrifying', 'and', 'acting', 'bad', 'Truly', '?', 'itself', 'of', 'out', 'fool', 'a', 'makes', 'NOISE', 'WHITE', 'that', 'place', 'first', 'the', 'in', 'made', 'movie', 'bad', 'so', 'gets', 'all', 'it', 'when', 'excitement', 'shocking', 'the', \"'s\", 'where', 'So', '.', 'uninteresting', 'and', 'undiscovered', ',', 'unexplained', 'up', 'ends', 'that', 'stuff', 'much', 'too', 'way', \"'s\", 'there', 'Because', '.', 'you', 'shocking', 'is', 'there', 'spot', 'every', 'about', 'misses', 'really', 'it', 'but', ',', 'thriller', 'shocking', 'pretty', 'a', 'be', 'to', 'supposed', 'is', 'this', ',', '/>Well', '/><br', '<br', '...', 'living', 'the', 'with', 'contact', 'making', 'tries', 'that', 'people', 'dead', 'is', 'like', 'seems', 'that', 'frequencies', 'radio', 'up', 'picks', 'that', 'husband', 'widowed', 'a', 'as', 'stars', 'he', ',', 'Here', '.', 'villains', 'great', 'his', 'of', 'shadows', 'the', 'in', 'falls', 'always', 'he', 'movies', 'Batman', 'Burton', 'Tim', 'the', 'in', ';', 'actor', 'good', 'a', 'been', 'never', 'really', 'has', 'Keaton', 'Michael']\n",
            "-----\n",
            "['.', 'blood', 'mind', \"n't\", 'does', 'and', ',', 'drama', 'likes', 'who', 'anyone', 'for', 'this', 'recomend', 'I', '.', 'score', 'wounderfull', 'a', 'had', 'also', 'movie', 'This', '.', 'time', 'the', 'all', 'to', 'use', 'still', 'i', 'that', 'quotes', 'memorable', 'very', 'some', 'had', 'and', ',', 'deal', 'good', 'a', 'story', 'the', 'to', 'added', 'it', ',', 'narration', 'the', 'enjoyed', 'much', 'very', 'also', 'I', '.', 'believable', 'more', 'the', 'all', 'story', 'the', 'making', ',', 'strengths', 'and', ',', 'weaknesses', 'has', 'He', '.', 'you', 'surprise', 'to', 'fails', 'never', 'he', 'because', ',', 'movie', 'the', 'throughout', 'of', 'tired', 'get', \"n't\", 'do', 'you', 'that', 'performance', 'brilliant', 'A', '.', 'Hutton', 'Timothy', 'psychotic', 'the', 'to', 'compared', 'nothing', 'but', ',', 'great', 'was', 'David', '.', 'character', 'particular', 'that', 'playing', 'else', 'anyone', 'see', 'not', 'could', 'I', 'because', 'also', ',', 'role', 'his', 'for', 'credit', 'extra', 'him', 'give', 'I', ',', 'Therefore', '.', 'to', 'used', 'are', 'we', 'Molder', 'Agent', 'the', 'playing', 'of', 'instead', ',', 'new', 'something', 'with', 'consistent', 'remain', 'to', 'managing', ',', 'well', 'very', 'character', 'his', 'played', 'Duchovney', '.', 'for', 'hoped', 'had', 'I', 'what', 'to', 'up', 'lived', 'movie', 'This', '.', 'proud', 'me', 'made', 'he', 'and', ',', 'did', 'he', 'finally', 'and', ',', 'business', 'movie', 'the', 'into', 'go', 'to', 'Duchovney', 'David', 'wanted', 'always', \"'d\", 'I']\n",
            "['...', 'here', 'well', 'too', 'work', \"n't\", 'does', 'and', 'since', 'often', 'too', 'seen', \"'ve\", 'we', 'one', 'is', 'episode', \"'\", 'house', 'the', 'of', 'destruction', \"'\", 'climactic', 'the', ',', 'hand', 'other', 'the', 'On', '.', 'point', 'plot', 'major', 'a', 'but', 'one', 'good', 'a', 'only', 'not', 'is', 'gag', 'bull', 'snoring', ',', 'sleepy', 'the', 'and', 'here', 'asset', 'major', 'a', 'is', 'scenery', 'Arizona', \"'\", 'Western', \"'\", 'typically', 'the', ',', 'expected', 'be', 'can', 'As', '!', 'backs', 'their', 'behind', 'skills', 'womanizing', 'his', 'practices', 'Presley', 'while', 'steadily', 'bottle', 'the', 'hit', 'Indians', 'male', 'the', 'were', 'parties', 'all', '-', 'for', '-', 'free', 'of', 'shortage', 'no', 'be', 'to', 'seems', 'there', ',', 'songs', 'of', 'lack', ')', 'blandness', 'usual', 'their', 'given', 'unwelcome', 'not', 'if', '(', 'surprising', 'a', 'is', 'there', '/>While', '/><br', 'br', '<', '!', 'day', 'present', 'the', 'in', 'clothes', 'chieftain', 'old', 'his', 'donning', 'still', 'father', 'stubborn', \"'s\", 'Meredith', 'is', 'who', 'Thundercloud', 'Chief', 'sarcastic', 'as', 'amusing', 'particularly', 'is', 'who', 'Gomez', 'Thomas', 'and', '-', 'L.Q.', 'and', 'Henry', '\\x96', 'Jones', 'two', 'the', ',', ')', 'wife', 'Mexican', \"'s\", 'Meredith', 'as', '(', 'Jurado', 'Katy', 'like', 'talent', 'Hollywood', 'veteran', 'top', 'by', 'surrounded', 'Elvis', 'see', 'to', 'glad', 'is', 'one', 'but', '-', 'too', 'Presley', 'Mr.', 'on', 'eye', 'her', 'has', 'who', 'bartender', 'bawdy', 'a', 'and', 'character', \"'\", 'Elvis', 'to', 'father', 'Indian', 'dopey', 'a', ',', 'respectively', ',', 'as', '\\x96', 'embarrassing', 'indeed', 'are', 'Blondell', 'Joan', 'and', 'Meredith', 'Burgess', ',', 'Sure', '.', 'considering', 'bad', 'that', 'all', 'not', 'is', 'and', 'misfire', 'interesting', 'an', 'as', 'emerges', ',', 'best', 'at', ',', 'which', 'JOE', ',', 'AWAY', 'STAY', 'was', 'that', 'misstep', 'the', 'for', 'him', 'forgive', 'can', 'I', 'personally', ',', 'Therefore', '.', 'LITTLE', 'A', 'LOVE', ',', 'LITTLE', 'A', 'LIVE', 'and', 'SPEEDWAY', '\\x96', 'well', 'as', 'movies', 'decent', 'pretty', 'two', 'in', 'starred', 'and', 'father', 'a', 'became', 'also', 'he', 'but', 'Special', 'TV', '\"', 'Comeback', '\"', 'celebrated', 'that', 'make', 'he', 'did', 'only', 'not', ':', 'Elvis', 'for', 'time', 'swell', 'pretty', 'a', 'was', '1968', ',', 'years', 'lean', 'few', 'a', 'After']\n",
            "['!', 'waiting', 'be', \"'ll\", 'I', '!', '\"', 'can', 'you', 'as', 'sexy', 'as', 'it', 'Make', '\"', ',', 'this', 'reading', 'are', 'any', 'If', '.', 'sixties', 'the', 'of', 'films', 'the', 'to', 'homage', 'good', 'a', 'make', 'to', 'director', 'a', 'for', 'waiting', 'continue', 'to', 'have', 'to', 'going', \"'m\", 'I', 'guess', 'I', '.', 'well', 'as', 'appear', 'Stockwell', 'Dean', 'and', 'Giannini', 'Giancarlo', ',', 'Depardieu', 'Gerard', '.', ')', 'Barbarella', '(', 'Law', 'Phillip', 'John', 'its', 'and', 'sixties', 'the', 'from', 'popular', 'was', 'who', 'film', 'this', 'in', 'actor', 'one', 'is', 'There', '.', 'them', 'in', 'appeared', 'who', 'actors', 'by', 'cameos', 'of', 'lots', 'have', 'could', 'They', '.', 'films', 'early', 'those', 'to', 'homage', 'an', 'been', 'have', 'could', 'film', 'the', 'wish', 'I', '.', 'script', 'the', 'into', 'it', 'puts', 'and', 'days', 'early', 'his', 'from', 'exploits', 'fathers', 'his', 'of', 'some', 'uses', 'who', 'Coppola', 'Roman', 'by', 'directed', 'and', 'written', 'is', 'Film', '.', 'it', 'in', 'barely', \"'s\", 'she', 'but', 'Dragonfly', 'plays', 'who', 'film', 'the', 'in', 'woman', 'beautiful', 'real', 'one', 'is', \"'re\", 'They', '.', 'picture', 'the', 'complete', 'to', ')', 'Davies', 'Jeremy', '(', 'filmmaker', 'aspiring', 'young', 'a', 'let', 'to', 'decide', 'producers', 'the', 'So', '.', 'fired', 'is', 'director', 'the', 'and', 'made', 'being', '\"', 'Dragonfly', '\"', 'called', 'film', 'fi', '-', 'sci', 'European', 'a', 'has', 'here', 'story', 'The', '.', 'enough', 'nostalgic', \"n't\", 'was', 'it', 'because', 'was', 'film', 'this', 'by', 'disappointed', 'was', 'I', 'reason', 'The', '.', ')', '?', '\"', 'Candy', '\"', 'Remember', '(', 'act', 'could', 'they', 'if', 'matter', \"n't\", 'did', 'it', 'and', 'films', 'these', 'in', 'be', 'to', 'women', 'sexiest', 'the', 'found', 'They', '.', 'them', 'in', 'appeared', 'that', \"'s\", 'actress', 'the', 'of', 'because', 'its', 'and', 'sixties', 'the', 'of', 'films', 'fi', '-', 'sci', 'and', 'action', 'sexy', 'the', 'love', 'really', 'I']\n",
            "['.', 'to', 'points', 'three', 'give', 'only', 'could', 'I', 'that', 'movie', 'disappointing', 'another', 'again', '/>Once', '/><br', 'warehouse.<br', 'the', 'in', 'especially', ',', 'mood', 'the', 'set', 'not', 'did', 'scenes', 'certain', 'in', 'lighting', 'the', ',', 'mysterious', 'more', 'movie', 'this', 'make', 'to', ',', 'Also', '.', 'world', 'her', 'into', 'you', 'pulled', 'that', 'acting', 'her', 'or', 'character', 'her', 'in', 'depth', 'enough', 'not', 'was', 'There', '.', 'crimes', 'solve', 'to', 'desire', 'unquenchable', 'an', 'with', 'senior', 'school', 'high', 'a', 'as', 'unbelievable', 'was', ',', ')', 'Silverstone', 'Alicia', '(', 'Gordano', 'Mary', ',', 'character', 'main', 'The', '.', 'terrible', 'was', 'premise', 'The', '.', 'Period', '.', 'dud', 'a', 'just', 'was', 'movie', 'this', 'But', '.', 'releases', 'studio', 'major', 'the', 'to', 'criteria', 'tougher', 'apply', 'I', 'Therefore', '.', 'studios', 'their', 'and', 'productions', 'budget', 'big', 'from', 'flawlessness', 'near', 'expect', 'I', '.', 'productions', 'budget', 'big', 'the', 'versus', 'productions', 'budget', 'low', 'and', 'independent', 'to', 'weight', 'lower', 'a', 'apply', 'also', 'I', '.', 'movie', 'budget', 'low', 'a', 'view', 'I', 'when', 'objective', 'very', 'be', 'to', 'try', 'I']\n",
            "['.', 'on', 'later', 'in', 'you', 'drown', 'to', 'going', 'is', 'Disney', 'sequels', 'other', 'the', 'of', 'rest', 'the', 'for', 'prepare', 'can', 'you', 'Then', '.', 'at', 'laugh', 'and', 'rent', 'to', 'left', 'better', 'is', '2', 'MERMAID', 'LITTLE', ',', 'Disney', 'to', 'embarrassment', 'An', '.', 'handle', 'to', 'much', 'too', 'and', 'cramped', 'incredibly', 'seemed', 'everything', ',', 'minutes', '75', 'in', 'but', ',', 'away', 'anything', 'give', 'not', 'will', '/>I', '/><br', 'disappointment.<br', 'another', 'be', 'to', 'seemed', 'EVERYONE', 'and', 'Morgana', 'between', 'climax', 'The', '!', 'it', 'played', 'under', 'terribly', 'They', '.', 'chef', 'royal', 'the', ',', 'Louie', 'and', 'Sebastien', 'between', 'rematch', 'the', 'include', 'moments', 'disappointing', '/>Other', '/><br', 'KING!<br', 'LION', 'the', 'from', 'duo', 'another', 'of', 'you', 'remind', 'not', 'does', 'walrus', 'and', 'penguin', 'the', 'that', 'me', 'tell', 'not', 'Do', '!', 'itself', 'plagiarize', 'only', 'can', 'Disney', 'guess', 'I', '.', 'movies', 'Disney', 'other', 'straight', 'come', 'have', 'to', 'seemed', 'watch', 'you', 'everything', 'feeling', 'that', 'you', 'gives', '2', 'MERMAID', 'LITTLE', '.', 'sequel', 'video', 'to', 'direct', 'this', 'describes', 'what', 'is', 'terrible', 'and', '/>Awful', '/><br', 'there.<br', 'in', 'go', 'not', 'can', 'she', 'why', 'wondering', 'up', 'grows', 'Melody', 'while', 'ocean', 'the', 'around', 'wall', 'a', 'build', 'Eric', 'and', 'Ariel', ',', 'attack', 'the', 'Stopping', '.', 'trident', \"'s\", 'King', 'the', 'get', 'to', 'tool', 'defense', 'a', 'as', 'Melody', 'use', 'to', 'plans', 'who', 'Morgana', ',', 'sister', \"'s\", 'Ursula', 'by', 'crashed', 'quickly', 'is', 'celebration', 'The', '.', 'Triton', 'King', 'to', 'her', 'introducing', 'on', 'plan', 'Eric', 'and', 'Ariel', ',', 'Melody', 'daughter', 'their', 'of', 'birth', 'the', 'Celebrating', '.', 'off', 'ripped', 'blatantly', 'feel', 'will', 'MERMAID', 'LITTLE', 'original', 'the', 'seen', 'has', 'who', 'anybody', 'as', 'times', 'many', 'too', 'one', 'well', 'the', 'to', 'goes', 'Disney']\n",
            "['.', 'refresher', 'a', 'for', 'again', 'it', 'seeing', 'on', 'intention', 'no', 'have', 'I', 'but', ',', 'more', 'much', ',', 'much', \"'s\", 'there', 'that', 'sure', \"'m\", 'I', '.', ')', 'them', 'on', 'step', 'they', 'when', 'style', 'Jerry', 'and', 'Tom', 'up', 'stand', 'that', ',', 'lawn', 'the', 'in', 'laying', 'rakes', 'with', 'family', 'cannibal', 'entire', 'the', 'out', 'knocks', 'he', '(', 'watch', 'to', 'unbearable', 'nearly', 'was', 'it', 'that', 'acted', 'poorly', 'and', 'hokey', 'so', 'was', 'ending', 'dramatic', 'big', 'The', '.', 'earlier', 'years', '15', 'done', 'were', 'they', 'like', 'look', ',', 'are', 'there', 'few', 'what', ',', 'effects', 'special', 'The', '.', 'COMEDY', 'a', 'was', 'this', 'that', 'and', ',', 'FUNNY', 'be', 'to', 'trying', 'were', 'they', 'that', 'understand', 'to', 'while', 'a', 'me', 'took', 'it', 'that', 'bad', 'so', 'are', 'gags', 'the', ',', 'Firstly', '.', 'Puke', '.', 'after', 'ever', 'happily', 'live', 'they', ',', 'girlfriend', \"'s\", 'brother', 'the', 'with', 'off', 'runs', 'and', 'butts', 'their', 'all', 'kicks', ',', 'escape', 'dramatic', 'a', 'at', 'attempt', 'pathetic', ',', 'course', 'main', 'the', \"'s\", 'he', 'and', 'cannibals', 'all', \"'re\", 'they', ',', 'family', 'the', 'meet', 'to', 'home', 'him', 'takes', ',', 'him', 'in', 'interest', 'an', 'takes', 'who', 'bombshell', 'blonde', 'meets', ',', 'weekend', 'ski', 'on', 'goes', ',', 'altar', 'at', 'left', 'loser', 'Fat', '.', 'begin', 'to', 'where', ',', 'my', '/>Oh', '/><br', 'alert---<br', '/>---Spoiler', '/><br', 'clear.<br', 'crystal', 'still', 'is', 'theater', 'the', 'leaving', 'upon', 'attitude', 'my', 'but', ',', 'specifics', 'many', 'very', 'remember', \"n't\", 'ca', 'I', 'years', '15', 'after', 'and', ',', ')', 'there', 'people', '5', 'of', '3', 'were', 'buddies', 'two', 'my', 'and', 'myself', '(', 'theaters', 'the', 'in', 'was', 'it', 'when', 'it', 'see', 'to', 'went', 'I', '.', 'badness', \"'s\", 'movie', 'this', 'at', 'rave', 'to', 'just', 'IMDb', 'with', 'registered', 'I', 'and', ',', 'way', 'the', 'by', ',', 'review', 'movie', 'first', 'my', 'is', 'This', '.', 'angry', 'and', 'insulted', 'personally', 'feeling', 'from', 'away', 'walked', 'I', 'that', 'two', 'of', 'one', 'is', 'one', 'this', 'seeing', 'ever', 'remember', 'can', 'I', 'movies', '700', 'nearly', 'the', 'of', ',', 'However', '.', 'like', \"n't\", 'did', 'they', 'movie', 'any', 'virtually', 'about', 'that', 'say', 'to', 'tend', 'they', ',', '\"', 'seen', 'ever', \"'ve\", 'I', 'movie', 'worst', '\"', 'the', \"'s\", 'it', 'say', 'people', 'many', 'When']\n",
            "['.', 'Soho', 'in', 'theatre', 'viewing', 'little', 'a', 'in', 'once', 'it', 'saw', 'only', 'I', ',', 'again', 'film', 'the', 'see', 'to', 'love', 'also', 'would', 'I', '?', 'is', 'it', 'where', 'knows', 'anybody', 'if', 'again', 'script', 'original', 'my', 'see', 'to', 'love', 'would', 'I', '.', 'good', 'so', 'be', \"n't\", 'would', 'it', 'had', 'I', 'if', 'perhaps', ';', 'it', 'on', 'work', 'more', 'much', 'do', 'to', 'asked', 'be', 'to', 'expected', 'I', ',', 'draft', 'first', 'written', 'hurriedly', 'a', 'from', ',', 'piece', 'made', 'beautifully', 'a', \"'s\", 'It', '.', 'on', 'back', 'put', 'name', 'my', 'have', 'to', 'asked', 'quickly', 'very', 'I', 'that', 'impressed', 'so', 'was', 'I', '.', 'film', 'finished', 'the', 'see', 'to', 'invited', 'was', 'I', 'Then', '.', 'screenplay', 'my', 'to', 'done', 'have', 'might', 'they', 'what', 'idea', 'no', 'had', 'I', '.', 'off', 'name', 'my', 'took', 'and', 'circumstances', 'the', 'under', 'do', 'could', 'I', 'thing', 'only', 'the', 'did', 'I', 'So', '.', 'late', 'too', ',', 'told', 'was', 'I', '.', 'it', 'lost', 'had', 'I', 'because', 'script', 'original', 'my', 'see', 'I', 'could', 'I', 'if', 'asked', 'also', 'I', '.', 'draft', 'another', 'do', 'to', 'me', 'like', 'might', 'he', 'whether', ',', 'agent', 'my', 'through', ',', 'Collinson', 'Peter', 'asked', 'I', '.', 'floor', 'the', 'on', 'was', 'it', 'that', 'heard', 'I', 'when', 'worried', 'understandably', 'was', 'I', 'so', 'draft', 'first', 'a', 'written', 'only', 'had', 'I', '.', 'it', 'about', 'hear', 'to', 'got', 'I', ',', 'mine', 'of', 'friend', 'a', 'was', 'Dobie', 'Alan', 'Because', '.', 'me', 'telling', 'without', 'on', 'it', 'sold', 'who', 'producers', 'some', 'for', 'it', 'wrote', 'I', '.', 'film', 'the', 'see', 'to', 'asked', 'was', 'I', 'until', 'made', 'being', 'it', 'about', 'little', 'very', 'knew', 'and', 'screenplay', 'the', 'wrote', 'I', 'because', 'is', 'this', 'say', 'I', 'why', 'reason', 'The']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMFkQlu9wpTE"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "''' Use max of 25000 frequently occuring words to create vocabulary. Use pretrained GloVe embedding. Embedding of words that exist in the data but not in the\n",
        "dictionary are set using normal distribution.\n",
        "'''\n",
        "TEXT.build_vocab(train_data,\n",
        "                 max_size = MAX_VOCAB_SIZE,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxSc8D2j2SQe"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "'''Creates batches with samples of nearly same sizes. may not be exact'''\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True, \n",
        "    device = device)"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1QTKEoNbQ8u",
        "outputId": "dc1ac317-d785-4e74-e0df-01d1063795d4"
      },
      "source": [
        "for i in train_iterator:\n",
        "  print(i)\n",
        "  print('text size',i.text[0].shape, '\\n' 'text length tensor', i.text[1], i.text[1].shape, '\\n' 'label', i.label, i.label.shape)\n",
        "  break"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 64]\n",
            "\t[.text]:('[torch.cuda.LongTensor of size 132x64 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
            "text size torch.Size([132, 64]) \n",
            "text length tensor tensor([132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132,\n",
            "        132, 132, 132, 132, 132, 132, 132, 132, 132, 131, 131, 131, 131, 131,\n",
            "        131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
            "        131, 131, 131, 131, 131, 131, 130, 130, 130, 130, 130, 130, 130, 130,\n",
            "        130, 130, 130, 130, 130, 130, 130, 130], device='cuda:0') torch.Size([64]) \n",
            "label tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0.], device='cuda:0') torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyrnrsgCJq1"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "    super().__init__()\n",
        "    self.embedding= nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "    self.rnn = nn.LSTM(embedding_dim, \n",
        "                       hidden_dim,\n",
        "                       num_layers=n_layers,\n",
        "                       bidirectional= bidirectional,\n",
        "                       dropout=dropout)\n",
        "    self.rnn2 = nn.LSTM(hidden_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "\n",
        "    direction_count = 2 if bidirectional else 1\n",
        "\n",
        "    self.fc = nn.Linear(hidden_dim * direction_count, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.bidirectional=bidirectional\n",
        "\n",
        "  def forward(self, text, text_lengths ):\n",
        "    # print('text:',text_lengths)\n",
        "    embedded = self.dropout(self.embedding(text)); #  [sent len, batch size, embedded_dim]\n",
        "    # print('embedded',embedded.shape)\n",
        "\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,text_lengths)  # [sent length * batch size,  embedded_dim]\n",
        "    '''???? check how the dimension change is accounted for in RNN''' \n",
        "    # print('packed_embedded[0]',packed_embedded[0].shape)\n",
        "    packed_output,(hidden, cell)= self.rnn(packed_embedded)  #hidden = [num layers * num directions, batch size, hid dim] #cell = [num layers * num directions, batch size, hid dim]\n",
        "    # print('packed_output[0]',packed_output[0].shape,'hidden',hidden.shape, 'cell',cell.shape)\n",
        "    \n",
        "    '''For stacked LSTM, the hidden output of one block is fed as input to the next.\n",
        "        ?????  Just last layer or entire hidden block? What's the idea?'''\n",
        "    for i in range(2):\n",
        "      packed_output,(hidden, cell) = self.rnn2(hidden)\n",
        "\n",
        "    '''All layer hidden layers are stacked up. The last layer is the final output. In case of bidirectional RNN, there is a set of hidden layers for\n",
        "    each direction. These pairs are stacked up layerwise. \n",
        "    [Layer0_forward Layer0_backward, Layer1_forward Layer1_backward, Layer2_forward Layer2_backward ....]'''\n",
        "    if self.bidirectional:\n",
        "      ''' need to take final layer output and since bidirectional is true, take last two layers or last pair of layers'''\n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "      # print('bidirectional', hidden.shape)\n",
        "    else:\n",
        "      hidden = self.dropout(hidden[-1,:,:]) # hidden = [batch size, hid dim * num directions]\n",
        "      # print('unidirectional', hidden.shape)\n",
        "    return self.fc(hidden)\n"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU56E_k0S2zF"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM,\n",
        "           EMBEDDING_DIM,\n",
        "           HIDDEN_DIM,\n",
        "           OUTPUT_DIM,\n",
        "           N_LAYERS,\n",
        "           BIDIRECTIONAL,\n",
        "           DROPOUT,\n",
        "           PAD_IDX\n",
        "           )\n"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLRPc9L_URyu",
        "outputId": "8e77cdda-a394-48a5-bf48-1ae2d6c5b72d"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,446,057 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NTPy5giUt-4",
        "outputId": "78b661ad-1be2-41f9-ff88-771133040d75"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "print(UNK_IDX)\n",
        "\n",
        "'''set padding and unknown words embedding to zero to ensure RNN knows what to learn and where to stop'''\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n",
            "0\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.2146,  0.6712,  0.3821,  ...,  0.4095,  0.7454,  0.0046],\n",
            "        [-0.2187,  0.2048, -0.7215,  ...,  0.2931,  0.6471,  0.0721],\n",
            "        [-0.5528, -1.5618,  0.8585,  ..., -0.7364,  0.5901, -1.4599]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM55gDQ2WNPi"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEgta3dTXBF4"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4ofepYXXTlb"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    text, text_lengths = batch.text\n",
        "    predictions = model(text, text_lengths.cpu() ).squeeze(1)\n",
        "    loss = criterion(predictions, batch.label)\n",
        "    acc = binary_accuracy(predictions, batch.label)\n",
        "   \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss+=loss.item()\n",
        "    epoch_acc+=acc.item()\n",
        "    \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiT73nvEYnag"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      text, text_lengths = batch.text\n",
        "\n",
        "      predictions = model(text, text_lengths.cpu() ).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      acc = binary_accuracy(predictions, batch.label)\n",
        "      epoch_loss+=loss.item()\n",
        "      epoch_acc+=acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzILv0QZDyl"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0PXPv7mZKa7",
        "outputId": "2881ea9d-7158-4570-d135-2c773a24ed5b"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc= train(model,train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "  if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "  # break\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.689 | Train Acc: 52.92%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.85%\n",
            "Epoch: 02 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.81%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.15%\n",
            "Epoch: 03 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.85%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.15%\n",
            "Epoch: 04 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.97%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.15%\n",
            "Epoch: 05 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.68%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.15%\n",
            "Epoch: 06 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.57%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.15%\n",
            "Epoch: 07 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.691 | Train Acc: 52.39%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.15%\n",
            "Epoch: 08 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.686 | Train Acc: 54.49%\n",
            "\t Val. Loss: 0.671 |  Val. Acc: 58.34%\n",
            "Epoch: 09 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.609 | Train Acc: 68.07%\n",
            "\t Val. Loss: 0.440 |  Val. Acc: 83.51%\n",
            "Epoch: 10 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.327 | Train Acc: 87.01%\n",
            "\t Val. Loss: 0.315 |  Val. Acc: 88.09%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJG4LaRXaLW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e402df36-7daa-4f31-d3bb-32c9c14bcdc0"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.349 | Test Acc: 85.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmNVhIScaTDv"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    print('tokenized;',tokenized)\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    print('indexed;',indexed)\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    print('tensor;',tensor.shape)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    print('tensor unsqueezed;',tensor.shape,length)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXVqG8wJarmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432ad741-5058-4367-e215-9da64dcd9e4e"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized; ['This', 'film', 'is', 'terrible']\n",
            "indexed; [66, 24, 9, 442]\n",
            "tensor; torch.Size([4])\n",
            "tensor unsqueezed; torch.Size([4, 1]) [4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11247031390666962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXUd6oF4auS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34c3959-3eeb-4d1b-8da2-0a001e735bd7"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized; ['This', 'film', 'is', 'great']\n",
            "indexed; [66, 24, 9, 103]\n",
            "tensor; torch.Size([4])\n",
            "tensor unsqueezed; torch.Size([4, 1]) [4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9194293022155762"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Nlwy2Gg8Ok"
      },
      "source": [
        "## Observations\n",
        "Increasing number of layers in the RNN cell increases the number of fully connected layers, thereby exponentially increasing the number of parameters with no improvement in accuracy.\n",
        "\n",
        "Unidirectional RNN needs higher complexity to achieve good accuracy. Interestingly, training on reverse string also produces good results. \n",
        "\n",
        "\n",
        "Bidirectional RNN achieves higher accuracy with lower complexity and fewer epochs.\n"
      ]
    }
  ]
}